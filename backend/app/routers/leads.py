from typing import List, Optional, Dict, Any, Union
from enum import Enum
import csv
import io
import json
import asyncio
from datetime import datetime
from difflib import SequenceMatcher
from fastapi import APIRouter, Depends, HTTPException, status, Query, UploadFile, File, BackgroundTasks, Body
from fastapi.responses import Response, JSONResponse
from sqlmodel import Session, select, or_, and_, func
from pydantic import BaseModel, Field
from app.database import get_session
from app.models import (
    Lead, LeadCreate, LeadResponse, LeadStatus, User, UserRole,
    LeadComment, LeadCommentCreate, LeadCommentResponse,
    Account, AccountCreate, Contact, ContactCreate, Opportunity, OpportunityCreate, SalesStage
)
from app.dependencies import get_current_active_user, apply_ownership_filter, ensure_ownership, require_ownership, check_ownership, check_limit
from app.services.audit_service import log_convert
from app.services.enrichment_service import enrich_lead
from app.services.kpi_service import track_kpi_activity
from app.models import GoalMetricType
from app.utils.pdf_parser import extract_text_from_pdf, parse_linkedin_data_with_llm
from app.services.lead_scoring import calculate_lead_score, should_recalculate_score, calculate_icp_score, should_recalculate_icp_score
from app.services.insight_generator import generate_lead_insight
import pandas as pd
import logging

logger = logging.getLogger(__name__)

router = APIRouter()


# Modelos para filtros avan√ßados
class FilterOperator(str, Enum):
    """Operadores dispon√≠veis para filtros"""
    # Operadores num√©ricos e de data
    EQUALS = "equals"
    NOT_EQUALS = "not_equals"
    GREATER_THAN = "greater_than"
    LESS_THAN = "less_than"
    GREATER_THAN_OR_EQUAL = "greater_than_or_equal"
    LESS_THAN_OR_EQUAL = "less_than_or_equal"
    BETWEEN = "between"
    
    # Operadores de string
    CONTAINS = "contains"
    NOT_CONTAINS = "not_contains"
    STARTS_WITH = "starts_with"
    ENDS_WITH = "ends_with"
    
    # Operadores especiais
    IS_NULL = "is_null"
    IS_NOT_NULL = "is_not_null"
    IN = "in"
    NOT_IN = "not_in"


class LeadFilter(BaseModel):
    """Filtro individual para um campo"""
    field: str = Field(..., description="Nome do campo do Lead")
    operator: str = Field(..., description="Operador de compara√ß√£o")
    value: Optional[Union[str, int, float, bool, List[Any]]] = Field(None, description="Valor para compara√ß√£o")
    value2: Optional[Union[str, int, float]] = Field(None, description="Segundo valor (para operador BETWEEN)")


class LeadFiltersRequest(BaseModel):
    """Request com m√∫ltiplos filtros"""
    filters: List[LeadFilter] = Field(default_factory=list, description="Lista de filtros")
    logic: str = Field("AND", description="L√≥gica de combina√ß√£o: 'AND' ou 'OR'")
    search: Optional[str] = Field(None, description="Busca geral em name, email, company")
    status: Optional[LeadStatus] = None
    assigned_to: Optional[int] = None
    source: Optional[str] = None
    min_score: Optional[int] = None
    max_score: Optional[int] = None
    skip: int = 0
    limit: int = 100


# Mapeamento de campos e seus tipos
FIELD_TYPES: Dict[str, str] = {
    # Campos num√©ricos
    "id": "number",
    "score": "number",
    "capital_social": "number",
    "assigned_to": "number",
    "icp_score": "number",
    
    # Campos de data
    "data_abertura": "date",
    "data_situacao_cadastral": "date",
    "data_opcao_simples": "date",
    "data_exclusao_simples": "date",
    "last_contact": "date",
    "next_followup": "date",
    "created_at": "date",
    "updated_at": "date",
    
    # Campos booleanos
    "simples_nacional": "boolean",
    "is_hiring": "boolean",
    "is_advertising": "boolean",
    
    # Campos de enum/dropdown
    "status": "enum",
    "situacao_cadastral": "enum",
    "porte": "enum",
    "natureza_juridica": "enum",
    "uf": "enum",
    
    # Campos de string (padr√£o)
    # Todos os outros campos s√£o tratados como string
}


def get_field_type(field_name: str) -> str:
    """Retorna o tipo do campo"""
    return FIELD_TYPES.get(field_name, "string")


@router.get("/filter-fields")
async def get_filter_fields():
    """Retorna os campos dispon√≠veis para filtros e seus tipos"""
    fields_info = []
    
    # Campos b√°sicos
    basic_fields = [
        ("name", "string", "Nome"),
        ("email", "string", "E-mail"),
        ("phone", "string", "Telefone"),
        ("company", "string", "Empresa"),
        ("position", "string", "Cargo"),
        ("linkedin_url", "string", "LinkedIn"),
        ("status", "enum", "Status"),
        ("source", "string", "Origem"),
        ("score", "number", "Score"),
        ("owner_id", "number", "Respons√°vel"),
    ]
    
    # Campos Casa dos Dados
    fiscal_fields = [
        ("cnpj", "string", "CNPJ"),
        ("razao_social", "string", "Raz√£o Social"),
        ("nome_fantasia", "string", "Nome Fantasia"),
        ("data_abertura", "date", "Data de Abertura"),
        ("capital_social", "number", "Capital Social"),
        ("situacao_cadastral", "enum", "Situa√ß√£o Cadastral"),
        ("porte", "enum", "Porte"),
        ("natureza_juridica", "enum", "Natureza Jur√≠dica"),
        ("municipio", "string", "Munic√≠pio"),
        ("uf", "enum", "UF"),
        ("cnae_principal_descricao", "string", "CNAE Principal"),
        ("simples_nacional", "boolean", "Simples Nacional"),
    ]
    
    # Campos de enriquecimento
    enrichment_fields = [
        ("city", "string", "Cidade"),
        ("state", "string", "Estado"),
        ("industry", "string", "Ind√∫stria"),
        ("company_size", "string", "Tamanho da Empresa"),
    ]
    
    # Campos de Qualifica√ß√£o ICP
    icp_fields = [
        ("tech_stack", "string", "Stack Tecnol√≥gico"),
        ("is_hiring", "boolean", "Est√° Contratando"),
        ("is_advertising", "boolean", "Est√° Fazendo Publicidade"),
        ("icp_score", "number", "Score ICP"),
    ]
    
    all_fields = basic_fields + fiscal_fields + enrichment_fields + icp_fields
    
    for field_name, field_type, label in all_fields:
        operators = []
        
        if field_type == "number":
            operators = [
                {"value": "equals", "label": "Igual a"},
                {"value": "not_equals", "label": "Diferente de"},
                {"value": "greater_than", "label": "Maior que"},
                {"value": "less_than", "label": "Menor que"},
                {"value": "greater_than_or_equal", "label": "Maior ou igual a"},
                {"value": "less_than_or_equal", "label": "Menor ou igual a"},
                {"value": "between", "label": "Entre"},
                {"value": "is_null", "label": "√â nulo"},
                {"value": "is_not_null", "label": "N√£o √© nulo"},
            ]
        elif field_type == "date":
            operators = [
                {"value": "equals", "label": "Igual a"},
                {"value": "not_equals", "label": "Diferente de"},
                {"value": "greater_than", "label": "Depois de"},
                {"value": "less_than", "label": "Antes de"},
                {"value": "between", "label": "Entre"},
                {"value": "is_null", "label": "√â nulo"},
                {"value": "is_not_null", "label": "N√£o √© nulo"},
            ]
        elif field_type == "boolean":
            operators = [
                {"value": "equals", "label": "Igual a"},
                {"value": "not_equals", "label": "Diferente de"},
                {"value": "is_null", "label": "√â nulo"},
                {"value": "is_not_null", "label": "N√£o √© nulo"},
            ]
        elif field_type == "enum":
            operators = [
                {"value": "equals", "label": "Igual a"},
                {"value": "not_equals", "label": "Diferente de"},
                {"value": "in", "label": "Est√° em"},
                {"value": "not_in", "label": "N√£o est√° em"},
                {"value": "is_null", "label": "√â nulo"},
                {"value": "is_not_null", "label": "N√£o √© nulo"},
            ]
        else:  # string
            operators = [
                {"value": "equals", "label": "Igual a"},
                {"value": "not_equals", "label": "Diferente de"},
                {"value": "contains", "label": "Cont√©m"},
                {"value": "not_contains", "label": "N√£o cont√©m"},
                {"value": "starts_with", "label": "Come√ßa com"},
                {"value": "ends_with", "label": "Termina com"},
                {"value": "is_null", "label": "√â nulo"},
                {"value": "is_not_null", "label": "N√£o √© nulo"},
            ]
        
        fields_info.append({
            "field": field_name,
            "type": field_type,
            "label": label,
            "operators": operators
        })
    
    return {"fields": fields_info}


def calculate_similarity(str1: str, str2: str) -> float:
    """Calcula similaridade entre duas strings (0.0 a 1.0)"""
    if not str1 or not str2:
        return 0.0
    str1_clean = str1.strip().lower()
    str2_clean = str2.strip().lower()
    if str1_clean == str2_clean:
        return 1.0
    return SequenceMatcher(None, str1_clean, str2_clean).ratio()


def normalize_phone(phone: Optional[str]) -> Optional[str]:
    """Normaliza telefone removendo caracteres especiais"""
    if not phone:
        return None
    return ''.join(filter(str.isdigit, phone))


def normalize_email(email: Optional[str]) -> Optional[str]:
    """Normaliza email para compara√ß√£o"""
    if not email:
        return None
    return email.strip().lower()


@router.get("/analyze-duplicates")
async def analyze_duplicates(
    min_similarity: float = Query(0.85, ge=0.0, le=1.0, description="Similaridade m√≠nima para considerar duplicado (0.0 a 1.0)"),
    session: Session = Depends(get_session),
    current_user: User = Depends(get_current_active_user)
):
    """
    Analisa leads duplicados usando crit√©rios inteligentes:
    - Nome similar (fuzzy matching)
    - Email igual
    - CNPJ igual
    - Telefone igual
    - Nome + empresa similar
    """
    try:
        # Buscar todos os leads do tenant
        all_leads = session.exec(
            select(Lead).where(Lead.tenant_id == current_user.tenant_id)
        ).all()
        
        duplicates_groups: List[Dict[str, Any]] = []
        processed_ids = set()
        
        for i, lead1 in enumerate(all_leads):
            if lead1.id in processed_ids:
                continue
            
            duplicate_group = [lead1]
            reasons = []
            
            for j, lead2 in enumerate(all_leads[i+1:], start=i+1):
                if lead2.id in processed_ids:
                    continue
                
                is_duplicate = False
                duplicate_reasons = []
                
                # Crit√©rio 1: Email igual (normalizado)
                email1 = normalize_email(lead1.email)
                email2 = normalize_email(lead2.email)
                if email1 and email2 and email1 == email2:
                    is_duplicate = True
                    duplicate_reasons.append(f"Email igual: {email1}")
                
                # Crit√©rio 2: CNPJ igual
                cnpj1 = lead1.cnpj.replace('.', '').replace('/', '').replace('-', '') if lead1.cnpj else ''
                cnpj2 = lead2.cnpj.replace('.', '').replace('/', '').replace('-', '') if lead2.cnpj else ''
                if cnpj1 and cnpj2 and cnpj1 == cnpj2 and len(cnpj1) == 14:
                    is_duplicate = True
                    duplicate_reasons.append(f"CNPJ igual: {cnpj1}")
                
                # Crit√©rio 3: Telefone igual (normalizado)
                phone1 = normalize_phone(lead1.phone)
                phone2 = normalize_phone(lead2.phone)
                if phone1 and phone2 and phone1 == phone2 and len(phone1) >= 10:
                    is_duplicate = True
                    duplicate_reasons.append(f"Telefone igual: {phone1}")
                
                # Crit√©rio 4: Nome similar (fuzzy matching)
                name_similarity = calculate_similarity(lead1.name or '', lead2.name or '')
                if name_similarity >= min_similarity:
                    is_duplicate = True
                    duplicate_reasons.append(f"Nome similar ({name_similarity:.0%}): '{lead1.name}' ‚âà '{lead2.name}'")
                
                # Crit√©rio 5: Nome + Empresa similar
                if lead1.company and lead2.company:
                    company_similarity = calculate_similarity(lead1.company, lead2.company)
                    combined_similarity = (name_similarity + company_similarity) / 2
                    if combined_similarity >= min_similarity and name_similarity >= 0.7:
                        is_duplicate = True
                        duplicate_reasons.append(f"Nome + Empresa similar ({combined_similarity:.0%}): '{lead1.name} @ {lead1.company}' ‚âà '{lead2.name} @ {lead2.company}'")
                
                if is_duplicate:
                    duplicate_group.append(lead2)
                    if not reasons:
                        reasons = duplicate_reasons
                    else:
                        # Adicionar raz√µes √∫nicas
                        for reason in duplicate_reasons:
                            if reason not in reasons:
                                reasons.append(reason)
            
            # Se encontrou duplicados, adicionar ao grupo
            if len(duplicate_group) > 1:
                # Marcar todos os IDs como processados
                for lead in duplicate_group:
                    processed_ids.add(lead.id)
                
                # Ordenar por ID (mais antigo primeiro)
                duplicate_group.sort(key=lambda x: x.id)
                
                duplicates_groups.append({
                    "group_id": len(duplicates_groups) + 1,
                    "leads": [
                        {
                            "id": lead.id,
                            "name": lead.name,
                            "email": lead.email,
                            "phone": lead.phone,
                            "company": lead.company,
                            "cnpj": lead.cnpj,
                            "status": lead.status.value if lead.status else None,
                            "source": lead.source,
                            "created_at": lead.created_at.isoformat() if lead.created_at else None,
                            "score": lead.score,
                            "icp_score": lead.icp_score
                        }
                        for lead in duplicate_group
                    ],
                    "reasons": reasons,
                    "count": len(duplicate_group)
                })
        
        return {
            "success": True,
            "total_duplicate_groups": len(duplicates_groups),
            "total_duplicate_leads": sum(group["count"] for group in duplicates_groups),
            "groups": duplicates_groups
        }
        
    except Exception as e:
        logger.error(f"‚ùå [DUPLICATES] Erro ao analisar duplicados: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Erro ao analisar duplicados: {str(e)}"
        )


@router.post("/merge-duplicates")
async def merge_duplicates(
    lead_ids: List[int] = Body(..., description="IDs dos leads a serem mesclados"),
    keep_lead_id: int = Body(..., description="ID do lead a ser mantido (os outros ser√£o mesclados nele)"),
    session: Session = Depends(get_session),
    current_user: User = Depends(get_current_active_user)
):
    """
    Mescla leads duplicados em um √∫nico lead, mantendo o lead especificado e mesclando dados dos outros
    """
    try:
        if keep_lead_id not in lead_ids:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="O lead a ser mantido deve estar na lista de IDs"
            )
        
        # Buscar o lead principal
        main_lead = session.get(Lead, keep_lead_id)
        if not main_lead:
            raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Lead principal n√£o encontrado")
        
        require_ownership(main_lead, current_user)
        
        # Buscar leads a serem mesclados
        leads_to_merge = []
        for lead_id in lead_ids:
            if lead_id == keep_lead_id:
                continue
            lead = session.get(Lead, lead_id)
            if lead:
                require_ownership(lead, current_user)
                leads_to_merge.append(lead)
        
        # Mesclar dados dos outros leads no principal
        for lead in leads_to_merge:
            # Mesclar campos vazios do principal com dados dos outros
            if not main_lead.email and lead.email:
                main_lead.email = lead.email
            if not main_lead.phone and lead.phone:
                main_lead.phone = lead.phone
            if not main_lead.company and lead.company:
                main_lead.company = lead.company
            if not main_lead.cnpj and lead.cnpj:
                main_lead.cnpj = lead.cnpj
            if not main_lead.address and lead.address:
                main_lead.address = lead.address
            if not main_lead.city and lead.city:
                main_lead.city = lead.city
            if not main_lead.state and lead.state:
                main_lead.state = lead.state
            if not main_lead.website and lead.website:
                main_lead.website = lead.website
            if not main_lead.linkedin_url and lead.linkedin_url:
                main_lead.linkedin_url = lead.linkedin_url
            if not main_lead.notes and lead.notes:
                main_lead.notes = lead.notes
            elif lead.notes and main_lead.notes:
                # Combinar notas
                main_lead.notes += f"\n--- Notas do lead duplicado (ID: {lead.id}) ---\n{lead.notes}"
            
            # Manter o maior score
            if lead.score and (not main_lead.score or lead.score > main_lead.score):
                main_lead.score = lead.score
            
            # Manter o maior ICP score
            if lead.icp_score and (not main_lead.icp_score or lead.icp_score > main_lead.icp_score):
                main_lead.icp_score = lead.icp_score
            
            # Manter o status mais avan√ßado (se aplic√°vel)
            status_order = {
                'won': 8, 'lost': 7, 'negotiation': 6, 'proposal_sent': 5,
                'meeting_scheduled': 4, 'qualified': 3, 'contacted': 2, 'new': 1, 'nurturing': 0
            }
            if lead.status and main_lead.status:
                if status_order.get(lead.status.value, 0) > status_order.get(main_lead.status.value, 0):
                    main_lead.status = lead.status
            
            # Deletar o lead duplicado
            session.delete(lead)
        
        session.add(main_lead)
        session.commit()
        session.refresh(main_lead)
        
        logger.info(f"‚úÖ [DUPLICATES] {len(leads_to_merge)} leads mesclados no lead {keep_lead_id}")
        
        return {
            "success": True,
            "message": f"{len(leads_to_merge)} leads mesclados com sucesso",
            "merged_lead": {
                "id": main_lead.id,
                "name": main_lead.name,
                "email": main_lead.email,
                "phone": main_lead.phone,
                "company": main_lead.company
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå [DUPLICATES] Erro ao mesclar leads: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Erro ao mesclar leads: {str(e)}"
        )


@router.post("", response_model=LeadResponse)
async def create_lead(
    lead_data: LeadCreate,
    session: Session = Depends(get_session),
    current_user: User = Depends(get_current_active_user)
):
    """Create a new lead for the current tenant"""
    # Verificar limite antes de criar
    await check_limit("leads", session, current_user)
    # Preparar dados com ownership
    lead_dict = lead_data.dict()
    lead_dict = ensure_ownership(lead_dict, current_user)
    
    # Se assigned_to foi fornecido mas owner_id n√£o, usar assigned_to como owner_id
    if lead_dict.get("assigned_to") and not lead_dict.get("owner_id"):
        lead_dict["owner_id"] = lead_dict["assigned_to"]
    
    lead = Lead(
        **lead_dict,
        tenant_id=current_user.tenant_id
    )
    session.add(lead)
    session.commit()
    session.refresh(lead)
    
    # Calcular score automaticamente
    try:
        lead.score = calculate_lead_score(lead, session)
        session.add(lead)
        session.commit()
        session.refresh(lead)
    except Exception as score_error:
        logger.warning(f"‚ö†Ô∏è [SCORING] Erro ao calcular score na cria√ß√£o do lead {lead.id}: {score_error}")
        # N√£o falhar a opera√ß√£o principal se o scoring falhar
    
    # Calcular ICP score automaticamente
    try:
        lead.icp_score = calculate_icp_score(lead)
        session.add(lead)
        session.commit()
        session.refresh(lead)
    except Exception as icp_error:
        logger.warning(f"‚ö†Ô∏è [ICP SCORING] Erro ao calcular ICP score na cria√ß√£o do lead {lead.id}: {icp_error}")
        # N√£o falhar a opera√ß√£o principal se o ICP scoring falhar
    
    # Track KPI activity for lead creation
    try:
        completed_goals = track_kpi_activity(
            session=session,
            user_id=current_user.id,
            tenant_id=current_user.tenant_id,
            metric_type=GoalMetricType.LEADS_CREATED,
            value=1.0,
            entity_type='Lead',
            entity_id=lead.id
        )
        session.commit()
        if completed_goals:
            logger.info(f"üéØ [KPI] {len(completed_goals)} goal(s) completed by lead creation")
    except Exception as kpi_error:
        logger.warning(f"‚ö†Ô∏è [KPI] Error tracking activity: {kpi_error}")
        # N√£o falhar a opera√ß√£o principal se o tracking falhar
    
    return lead


@router.post("/filter", response_model=List[LeadResponse])
async def filter_leads(
    filters_request: LeadFiltersRequest = Body(...),
    session: Session = Depends(get_session),
    current_user: User = Depends(get_current_active_user)
):
    """Get leads with advanced filters"""
    logger.info(f"üîç [FILTERS] Recebida requisi√ß√£o de filtros: {len(filters_request.filters)} filtro(s), l√≥gica={filters_request.logic}")
    
    # Base query for counting - aplicar filtro de ownership
    count_query = select(func.count(Lead.id))
    count_query = apply_ownership_filter(count_query, Lead, current_user)
    
    # Query for data - aplicar filtro de ownership
    query = select(Lead)
    query = apply_ownership_filter(query, Lead, current_user)
    
    # Aplicar filtros avan√ßados
    filter_conditions = []
    
    # Filtros estruturados
    if filters_request.filters:
        for filter_obj in filters_request.filters:
            try:
                # Verificar se o campo existe
                if not hasattr(Lead, filter_obj.field):
                    logger.warning(f"Campo '{filter_obj.field}' n√£o existe no modelo Lead")
                    continue
                
                field = getattr(Lead, filter_obj.field)
                field_type = get_field_type(filter_obj.field)
                operator = filter_obj.operator
                value = filter_obj.value
                value2 = filter_obj.value2
                
                logger.info(f"Aplicando filtro: campo={filter_obj.field}, operador={operator}, valor={value}, tipo={field_type}")
                
                # Ignorar filtros sem valor (exceto para operadores is_null/is_not_null)
                if value is None and operator not in ["is_null", "is_not_null"]:
                    logger.warning(f"Filtro ignorado: campo '{filter_obj.field}' sem valor")
                    continue
                
                if operator == "equals":
                    if field_type == "string":
                        # Para strings, equals deve ser compara√ß√£o exata (case-insensitive)
                        if value:
                            filter_conditions.append(field.ilike(f"{value}"))
                    else:
                        filter_conditions.append(field == value)
                
                elif operator == "not_equals":
                    if field_type == "string":
                        # Para strings, not_equals deve ser compara√ß√£o exata (case-insensitive)
                        if value:
                            filter_conditions.append(~field.ilike(f"{value}"))
                    else:
                        filter_conditions.append(field != value)
                
                elif operator == "greater_than":
                    if value is not None:
                        filter_conditions.append(field > value)
                
                elif operator == "less_than":
                    if value is not None:
                        filter_conditions.append(field < value)
                
                elif operator == "greater_than_or_equal":
                    if value is not None:
                        filter_conditions.append(field >= value)
                
                elif operator == "less_than_or_equal":
                    if value is not None:
                        filter_conditions.append(field <= value)
                
                elif operator == "between":
                    if value is not None and value2 is not None:
                        filter_conditions.append(and_(field >= value, field <= value2))
                    else:
                        logger.warning(f"Filtro 'between' ignorado: valores incompletos")
                
                elif operator == "contains":
                    if value:
                        filter_conditions.append(field.ilike(f"%{value}%"))
                
                elif operator == "not_contains":
                    if value:
                        filter_conditions.append(~field.ilike(f"%{value}%"))
                
                elif operator == "starts_with":
                    if value:
                        filter_conditions.append(field.ilike(f"{value}%"))
                
                elif operator == "ends_with":
                    if value:
                        filter_conditions.append(field.ilike(f"%{value}"))
                
                elif operator == "is_null":
                    filter_conditions.append(field.is_(None))
                
                elif operator == "is_not_null":
                    filter_conditions.append(field.isnot(None))
                
                elif operator == "in":
                    if value:
                        if not isinstance(value, list):
                            value = [value]
                        filter_conditions.append(field.in_(value))
                
                elif operator == "not_in":
                    if value:
                        if not isinstance(value, list):
                            value = [value]
                        filter_conditions.append(~field.in_(value))
                
            except Exception as e:
                logger.error(f"Erro ao aplicar filtro {filter_obj.field}: {e}", exc_info=True)
                raise HTTPException(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    detail=f"Erro ao aplicar filtro no campo '{filter_obj.field}': {str(e)}"
                )
    
    # Filtros legados (compatibilidade)
    if filters_request.status:
        filter_conditions.append(Lead.status == filters_request.status)
    
    if filters_request.assigned_to:
        filter_conditions.append(Lead.assigned_to == filters_request.assigned_to)
    
    if filters_request.source:
        filter_conditions.append(Lead.source == filters_request.source)
    
    if filters_request.min_score is not None:
        filter_conditions.append(Lead.score >= filters_request.min_score)
    
    if filters_request.max_score is not None:
        filter_conditions.append(Lead.score <= filters_request.max_score)
    
    if filters_request.search:
        search_filter = or_(
            Lead.name.ilike(f"%{filters_request.search}%"),
            Lead.email.ilike(f"%{filters_request.search}%"),
            Lead.company.ilike(f"%{filters_request.search}%")
        )
        filter_conditions.append(search_filter)
    
    # Combinar filtros com l√≥gica AND ou OR
    if filter_conditions:
        logger.info(f"üîç [FILTERS] Aplicando {len(filter_conditions)} condi√ß√£o(√µes) com l√≥gica {filters_request.logic}")
        if filters_request.logic.upper() == "OR":
            combined_filter = or_(*filter_conditions)
        else:
            combined_filter = and_(*filter_conditions)
        
        query = query.where(combined_filter)
        count_query = count_query.where(combined_filter)
    else:
        logger.warning("üîç [FILTERS] Nenhuma condi√ß√£o de filtro aplicada")
    
    # Order by created_at descending (newest first)
    query = query.order_by(Lead.created_at.desc())
    
    # Get total count before pagination
    total_count = session.exec(count_query).one()
    
    # Apply pagination
    query = query.offset(filters_request.skip).limit(filters_request.limit)
    
    leads = session.exec(query).all()
    
    # Serialize leads properly (handles datetime objects)
    leads_data = []
    for lead in leads:
        lead_dict = lead.dict()
        # Convert datetime objects to ISO format strings
        for key, value in lead_dict.items():
            if isinstance(value, datetime):
                lead_dict[key] = value.isoformat()
        leads_data.append(lead_dict)
    
    # Return JSONResponse with total count in header
    response = JSONResponse(content=leads_data)
    response.headers["X-Total-Count"] = str(total_count)
    return response


@router.get("", response_model=List[LeadResponse])
async def get_leads(
    status: Optional[LeadStatus] = Query(None, description="Filter by status"),
    search: Optional[str] = Query(None, description="Search in name, email, company"),
    assigned_to: Optional[int] = Query(None, description="Filter by assigned user"),
    source: Optional[str] = Query(None, description="Filter by source"),
    min_score: Optional[int] = Query(None, description="Minimum score"),
    max_score: Optional[int] = Query(None, description="Maximum score"),
    skip: int = Query(0, ge=0),
    limit: int = Query(100, ge=1, le=1000),
    session: Session = Depends(get_session),
    current_user: User = Depends(get_current_active_user)
):
    """Get all leads for the current tenant with filters (legacy endpoint)"""
    # Base query for counting - aplicar filtro de ownership
    count_query = select(func.count(Lead.id))
    count_query = apply_ownership_filter(count_query, Lead, current_user)
    
    # Query for data - aplicar filtro de ownership
    query = select(Lead)
    query = apply_ownership_filter(query, Lead, current_user)
    
    # Apply filters
    filters = []
    
    if status:
        filters.append(Lead.status == status)
    
    if assigned_to:
        filters.append(Lead.assigned_to == assigned_to)
    
    if source:
        filters.append(Lead.source == source)
    
    if min_score is not None:
        filters.append(Lead.score >= min_score)
    
    if max_score is not None:
        filters.append(Lead.score <= max_score)
    
    if search:
        search_filter = or_(
            Lead.name.ilike(f"%{search}%"),
            Lead.email.ilike(f"%{search}%"),
            Lead.company.ilike(f"%{search}%")
        )
        filters.append(search_filter)
    
    if filters:
        query = query.where(and_(*filters))
        count_query = count_query.where(and_(*filters))
    
    # Order by created_at descending (newest first)
    query = query.order_by(Lead.created_at.desc())
    
    # Get total count before pagination
    total_count = session.exec(count_query).one()
    
    # Apply pagination
    query = query.offset(skip).limit(limit)
    
    leads = session.exec(query).all()
    
    # Return leads - FastAPI will serialize them via response_model
    # We need to add the total count to the response headers
    from fastapi import Response
    from fastapi.responses import JSONResponse
    
    # Serialize leads properly (handles datetime objects)
    leads_data = []
    for lead in leads:
        lead_dict = lead.dict()
        # Convert datetime objects to ISO format strings
        for key, value in lead_dict.items():
            if isinstance(value, datetime):
                lead_dict[key] = value.isoformat()
        leads_data.append(lead_dict)
    
    # Return JSONResponse with total count in header
    response = JSONResponse(content=leads_data)
    response.headers["X-Total-Count"] = str(total_count)
    return response


@router.get("/debug/ownership", response_model=dict)
async def debug_ownership(
    session: Session = Depends(get_session),
    current_user: User = Depends(get_current_active_user)
):
    """Debug endpoint para verificar ownership dos leads"""
    from sqlmodel import text
    
    # Total de leads no tenant
    total_query = text("SELECT COUNT(*) FROM lead WHERE tenant_id = :tenant_id")
    total = session.exec(total_query.params(tenant_id=current_user.tenant_id)).first()
    
    # Leads sem owner_id
    null_owner_query = text("SELECT COUNT(*) FROM lead WHERE tenant_id = :tenant_id AND owner_id IS NULL")
    null_owner = session.exec(null_owner_query.params(tenant_id=current_user.tenant_id)).first()
    
    # Leads do usu√°rio atual
    user_leads_query = text("SELECT COUNT(*) FROM lead WHERE tenant_id = :tenant_id AND owner_id = :user_id")
    user_leads = session.exec(user_leads_query.params(tenant_id=current_user.tenant_id, user_id=current_user.id)).first()
    
    # Leads que o usu√°rio pode ver (com filtro de ownership)
    query = select(Lead)
    query = apply_ownership_filter(query, Lead, current_user)
    accessible_leads = len(session.exec(query).all())
    
    return {
        "user_id": current_user.id,
        "user_email": current_user.email,
        "user_role": current_user.role.value,
        "tenant_id": current_user.tenant_id,
        "total_leads_in_tenant": total[0] if total else 0,
        "leads_without_owner_id": null_owner[0] if null_owner else 0,
        "leads_owned_by_user": user_leads[0] if user_leads else 0,
        "leads_accessible_to_user": accessible_leads,
        "is_admin": current_user.role == UserRole.ADMIN
    }


@router.get("/stats/summary", response_model=dict)
async def get_leads_stats(
    session: Session = Depends(get_session),
    current_user: User = Depends(get_current_active_user)
):
    """Get leads statistics for the current tenant"""
    # Aplicar filtro de ownership
    query = select(Lead)
    query = apply_ownership_filter(query, Lead, current_user)
    all_leads = session.exec(query).all()
    
    stats = {
        "total": len(all_leads),
        "by_status": {},
        "by_source": {},
        "average_score": 0,
        "assigned": 0,
        "unassigned": 0
    }
    
    total_score = 0
    leads_with_score = 0
    
    for lead in all_leads:
        # Count by status
        status_key = lead.status.value if isinstance(lead.status, LeadStatus) else lead.status
        stats["by_status"][status_key] = stats["by_status"].get(status_key, 0) + 1
        
        # Count by source
        if lead.source:
            stats["by_source"][lead.source] = stats["by_source"].get(lead.source, 0) + 1
        
        # Calculate average score
        if lead.score is not None:
            total_score += lead.score
            leads_with_score += 1
        
        # Count assigned/unassigned
        if lead.assigned_to:
            stats["assigned"] += 1
        else:
            stats["unassigned"] += 1
    
    if leads_with_score > 0:
        stats["average_score"] = round(total_score / leads_with_score, 2)
    
    return stats


@router.get("/import-template")
async def download_import_template():
    """Download CSV template for lead import"""
    
    # Create CSV content
    output = io.StringIO()
    writer = csv.writer(output)
    
    # Write header
    writer.writerow([
        'Nome',
        'Empresa',
        'Cargo',
        'Linkedin',
        'Data 1o contato',
        'Status',
        'Pr√≥xima a√ß√£o',
        'Observa√ß√£o'
    ])
    
    # Write example rows
    writer.writerow([
        'Jo√£o Silva',
        'Tech Solutions',
        'CEO',
        'https://www.linkedin.com/in/joaosilva',
        '22/01/2024',
        'Lead Novo',
        'Enviar conex√£o com nota',
        'Interessado em automa√ß√£o'
    ])
    writer.writerow([
        'Maria Santos',
        'Inova√ß√£o Digital',
        'CTO',
        'https://www.linkedin.com/in/mariasantos',
        '22/01/2024',
        'Lead Novo',
        'Enviar conex√£o com nota',
        ''
    ])
    
    csv_content = output.getvalue()
    output.close()
    
    return Response(
        content=csv_content.encode('utf-8-sig'),  # BOM for Excel compatibility
        media_type='text/csv',
        headers={
            'Content-Disposition': 'attachment; filename="template_importacao_leads.csv"'
        }
    )


async def process_lead_enrichment(lead_id: int, cnpj: str):
    """Processa o enriquecimento de um lead em background"""
    import logging
    logger = logging.getLogger(__name__)
    
    try:
        # Criar nova sess√£o para o background task
        from app.database import engine
        from sqlmodel import Session
        with Session(engine) as db:
            try:
                lead = db.get(Lead, lead_id)
                if lead and cnpj:
                    enriched_lead = await enrich_lead(lead, cnpj)
                    db.add(enriched_lead)
                    db.commit()
                    db.refresh(enriched_lead)
                    logger.info(f"‚úÖ [BACKGROUND] Lead {lead_id} enriquecido com sucesso")
                else:
                    logger.warning(f"‚ö†Ô∏è [BACKGROUND] Lead {lead_id} n√£o encontrado ou CNPJ inv√°lido")
            except Exception as db_error:
                db.rollback()
                logger.error(f"‚ùå [BACKGROUND] Erro no banco ao enriquecer lead {lead_id}: {db_error}")
    except Exception as e:
        logger.error(f"‚ùå [BACKGROUND] Erro ao enriquecer lead {lead_id}: {e}")
        import traceback
        traceback.print_exc()
        import traceback
        traceback.print_exc()


@router.post("/import-csv")
async def import_leads_csv(
    file: UploadFile = File(...),
    background_tasks: BackgroundTasks = BackgroundTasks(),
    session: Session = Depends(get_session),
    current_user: User = Depends(get_current_active_user)
):
    """
    Import leads from CSV file (formato Casa dos Dados)
    Processa o CSV com pandas e inicia enriquecimento ag√™ntico para cada linha
    """
    if not file.filename.endswith('.csv'):
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="File must be a CSV file"
        )
    
    try:
        # Ler arquivo com pandas
        contents = await file.read()
        df = pd.read_csv(io.BytesIO(contents), encoding='utf-8-sig', dtype=str)
        
        logger.info(f"üìä [IMPORT] CSV lido com sucesso. {len(df)} linhas encontradas")
        logger.info(f"üìä [IMPORT] Colunas: {list(df.columns)}")
        
        imported = 0
        errors = []
        leads_to_enrich = []  # Lista de (lead_id, cnpj) para enriquecimento
        
        for idx, row in df.iterrows():
            try:
                row_num = idx + 2  # +2 porque idx come√ßa em 0 e linha 1 √© header
                
                # Extrair CNPJ (campo obrigat√≥rio para enriquecimento)
                cnpj = None
                for col in ['cnpj', 'CNPJ', 'Cnpj']:
                    if col in row and pd.notna(row[col]):
                        cnpj = str(row[col]).strip().replace('.', '').replace('/', '').replace('-', '')
                        break
                
                if not cnpj or len(cnpj) != 14:
                    errors.append(f"Linha {row_num}: CNPJ inv√°lido ou n√£o encontrado")
                    continue
                
                # Verificar se lead j√° existe pelo CNPJ
                existing_lead = session.exec(
                    select(Lead).where(
                        and_(
                            Lead.tenant_id == current_user.tenant_id,
                            Lead.cnpj == cnpj
                        )
                    )
                ).first()
                
                if existing_lead:
                    logger.info(f"üìã [IMPORT] Lead com CNPJ {cnpj} j√° existe. Atualizando...")
                    lead = existing_lead
                else:
                    # Criar novo lead
                    # Extrair nome (pode ser do primeiro s√≥cio ou razao_social)
                    name = None
                    for col in ['nome', 'Nome', 'NOME', 'name', 'Name']:
                        if col in row and pd.notna(row[col]):
                            name = str(row[col]).strip()
                            break
                    
                    # Se n√£o tem nome, usar razao_social
                    if not name:
                        for col in ['razao_social', 'Razao Social', 'RAZAO_SOCIAL']:
                            if col in row and pd.notna(row[col]):
                                name = str(row[col]).strip()
                                break
                    
                    if not name:
                        errors.append(f"Linha {row_num}: Nome n√£o encontrado")
                        continue
                    
                    lead = Lead(
                        tenant_id=current_user.tenant_id,
                        name=name,
                        cnpj=cnpj,
                        source='CSV Import - Casa dos Dados'
                    )
                    session.add(lead)
                    session.flush()  # Para obter o ID
                
                # Mapear colunas do CSV para campos do Lead
                # Raz√£o Social
                for col in ['razao_social', 'Razao Social', 'RAZAO_SOCIAL', 'razaoSocial']:
                    if col in row and pd.notna(row[col]):
                        lead.razao_social = str(row[col]).strip()
                        if not lead.company:
                            lead.company = lead.razao_social
                        break
                
                # Nome Fantasia
                for col in ['nome_fantasia', 'Nome Fantasia', 'NOME_FANTASIA', 'nomeFantasia']:
                    if col in row and pd.notna(row[col]):
                        lead.nome_fantasia = str(row[col]).strip()
                        break
                
                # Data de Abertura
                for col in ['data_abertura', 'Data Abertura', 'DATA_ABERTURA', 'dataAbertura']:
                    if col in row and pd.notna(row[col]):
                        try:
                            date_str = str(row[col]).strip()
                            # Tentar diferentes formatos
                            for fmt in ['%Y-%m-%d', '%d/%m/%Y', '%d-%m-%Y', '%Y/%m/%d']:
                                try:
                                    lead.data_abertura = datetime.strptime(date_str, fmt)
                                    break
                                except ValueError:
                                    continue
                        except Exception:
                            pass
                        break
                
                # Capital Social
                for col in ['capital_social', 'Capital Social', 'CAPITAL_SOCIAL', 'capitalSocial']:
                    if col in row and pd.notna(row[col]):
                        try:
                            capital_str = str(row[col]).strip().replace(',', '.').replace('R$', '').strip()
                            lead.capital_social = float(capital_str)
                        except (ValueError, TypeError):
                            pass
                        break
                
                # Situa√ß√£o Cadastral
                for col in ['situacao_cadastral', 'Situa√ß√£o Cadastral', 'SITUACAO_CADATRAL']:
                    if col in row and pd.notna(row[col]):
                        lead.situacao_cadastral = str(row[col]).strip()
                        break
                
                # Endere√ßo
                for col in ['logradouro', 'Logradouro', 'LOGRADOURO']:
                    if col in row and pd.notna(row[col]):
                        lead.logradouro = str(row[col]).strip()
                        break
                
                for col in ['numero', 'N√∫mero', 'NUMERO', 'numero']:
                    if col in row and pd.notna(row[col]):
                        lead.numero = str(row[col]).strip()
                        break
                
                for col in ['bairro', 'Bairro', 'BAIRRO']:
                    if col in row and pd.notna(row[col]):
                        lead.bairro = str(row[col]).strip()
                        break
                
                for col in ['cep', 'CEP', 'Cep']:
                    if col in row and pd.notna(row[col]):
                        cep = str(row[col]).strip().replace('-', '').replace('.', '')
                        lead.cep = cep
                        lead.zip_code = cep
                        break
                
                for col in ['municipio', 'Munic√≠pio', 'MUNICIPIO', 'munic√≠pio']:
                    if col in row and pd.notna(row[col]):
                        lead.municipio = str(row[col]).strip()
                        lead.city = lead.municipio
                        break
                
                for col in ['uf', 'UF', 'Uf', 'estado', 'Estado']:
                    if col in row and pd.notna(row[col]):
                        lead.uf = str(row[col]).strip().upper()
                        lead.state = lead.uf
                        break
                
                # Complemento
                for col in ['complemento', 'Complemento', 'COMPLEMENTO']:
                    if col in row and pd.notna(row[col]):
                        lead.complemento = str(row[col]).strip()
                        break
                
                # Situa√ß√£o Cadastral (campos adicionais)
                for col in ['data_situacao_cadastral', 'Data Situa√ß√£o Cadastral', 'dataSituacaoCadastral']:
                    if col in row and pd.notna(row[col]):
                        try:
                            date_str = str(row[col]).strip()
                            for fmt in ['%Y-%m-%d', '%d/%m/%Y', '%d-%m-%Y', '%Y/%m/%d']:
                                try:
                                    lead.data_situacao_cadastral = datetime.strptime(date_str, fmt)
                                    break
                                except ValueError:
                                    continue
                        except Exception:
                            pass
                        break
                
                for col in ['motivo_situacao_cadastral', 'Motivo Situa√ß√£o Cadastral', 'motivoSituacaoCadastral']:
                    if col in row and pd.notna(row[col]):
                        lead.motivo_situacao_cadastral = str(row[col]).strip()
                        break
                
                # Natureza Jur√≠dica
                for col in ['natureza_juridica', 'Natureza Jur√≠dica', 'NATUREZA_JURIDICA', 'naturezaJuridica']:
                    if col in row and pd.notna(row[col]):
                        lead.natureza_juridica = str(row[col]).strip()
                        break
                
                # Porte
                for col in ['porte', 'Porte', 'PORTE']:
                    if col in row and pd.notna(row[col]):
                        lead.porte = str(row[col]).strip()
                        break
                
                # CNAE Principal
                for col in ['cnae_principal_codigo', 'CNAE Principal C√≥digo', 'cnaePrincipalCodigo', 'cnae_principal', 'CNAE Principal']:
                    if col in row and pd.notna(row[col]):
                        lead.cnae_principal_codigo = str(row[col]).strip()
                        break
                
                for col in ['cnae_principal_descricao', 'CNAE Principal Descri√ß√£o', 'cnaePrincipalDescricao']:
                    if col in row and pd.notna(row[col]):
                        lead.cnae_principal_descricao = str(row[col]).strip()
                        if not lead.industry:
                            lead.industry = lead.cnae_principal_descricao
                        break
                
                # CNAEs Secund√°rios
                for col in ['cnaes_secundarios', 'CNAEs Secund√°rios', 'CNAES_SECUNDARIOS', 'cnaesSecundarios', 'cnaes_secundarios_json']:
                    if col in row and pd.notna(row[col]):
                        cnaes_value = str(row[col]).strip()
                        try:
                            json.loads(cnaes_value)
                            lead.cnaes_secundarios_json = cnaes_value
                        except:
                            lead.cnaes_secundarios_json = cnaes_value
                        break
                
                # Telefone e Email da Empresa
                for col in ['telefone_empresa', 'Telefone Empresa', 'TELEFONE_EMPRESA', 'telefoneEmpresa', 'telefone', 'Telefone']:
                    if col in row and pd.notna(row[col]):
                        lead.telefone_empresa = str(row[col]).strip()
                        if not lead.phone:
                            lead.phone = lead.telefone_empresa
                        break
                
                for col in ['email_empresa', 'Email Empresa', 'EMAIL_EMPRESA', 'emailEmpresa', 'email', 'Email']:
                    if col in row and pd.notna(row[col]):
                        lead.email_empresa = str(row[col]).strip()
                        if not lead.email:
                            lead.email = lead.email_empresa
                        break
                
                # Simples Nacional
                for col in ['simples_nacional', 'Simples Nacional', 'SIMPLES_NACIONAL', 'simplesNacional', 'optante_simples']:
                    if col in row and pd.notna(row[col]):
                        simples_value = str(row[col]).strip().lower()
                        lead.simples_nacional = simples_value in ['true', '1', 'sim', 'yes', 's']
                        break
                
                for col in ['data_opcao_simples', 'Data Op√ß√£o Simples', 'dataOpcaoSimples']:
                    if col in row and pd.notna(row[col]):
                        try:
                            date_str = str(row[col]).strip()
                            for fmt in ['%Y-%m-%d', '%d/%m/%Y', '%d-%m-%Y', '%Y/%m/%d']:
                                try:
                                    lead.data_opcao_simples = datetime.strptime(date_str, fmt)
                                    break
                                except ValueError:
                                    continue
                        except Exception:
                            pass
                        break
                
                for col in ['data_exclusao_simples', 'Data Exclus√£o Simples', 'dataExclusaoSimples']:
                    if col in row and pd.notna(row[col]):
                        try:
                            date_str = str(row[col]).strip()
                            for fmt in ['%Y-%m-%d', '%d/%m/%Y', '%d-%m-%Y', '%Y/%m/%d']:
                                try:
                                    lead.data_exclusao_simples = datetime.strptime(date_str, fmt)
                                    break
                                except ValueError:
                                    continue
                        except Exception:
                            pass
                        break
                
                # S√≥cios (pode estar em formato JSON ou texto)
                for col in ['socios', 'S√≥cios', 'SOCIOS', 'socios_json', 'sociosJson']:
                    if col in row and pd.notna(row[col]):
                        socios_value = str(row[col]).strip()
                        # Se j√° √© JSON v√°lido, usar direto
                        try:
                            json.loads(socios_value)
                            lead.socios_json = socios_value
                        except:
                            # Se n√£o √© JSON, tentar converter
                            lead.socios_json = socios_value
                        break
                
                # Montar endere√ßo completo
                endereco_parts = []
                if lead.logradouro:
                    endereco_parts.append(lead.logradouro)
                if lead.numero:
                    endereco_parts.append(lead.numero)
                if lead.complemento:
                    endereco_parts.append(lead.complemento)
                if lead.bairro:
                    endereco_parts.append(lead.bairro)
                if lead.municipio:
                    endereco_parts.append(lead.municipio)
                if lead.uf:
                    endereco_parts.append(lead.uf)
                if lead.cep:
                    endereco_parts.append(f"CEP: {lead.cep}")
                
                if endereco_parts:
                    lead.address = ', '.join(endereco_parts)
                
                lead.updated_at = datetime.utcnow()
                session.add(lead)
                session.flush()
                
                imported += 1
                
                # Adicionar √† lista para enriquecimento em background
                if cnpj:
                    leads_to_enrich.append((lead.id, cnpj))
                
            except Exception as e:
                errors.append(f"Linha {row_num}: {str(e)}")
                logger.error(f"‚ùå [IMPORT] Erro na linha {row_num}: {e}")
                continue
        
        session.commit()
        
        # Iniciar enriquecimento em background para cada lead
        for lead_id, cnpj in leads_to_enrich:
            background_tasks.add_task(process_lead_enrichment, lead_id, cnpj, session)
            logger.info(f"üöÄ [IMPORT] Enriquecimento agendado para lead {lead_id} (CNPJ: {cnpj})")
        
        return {
            "message": f"Importa√ß√£o conclu√≠da. {len(leads_to_enrich)} lead(s) ser√£o enriquecidos em background.",
            "imported": imported,
            "enrichment_scheduled": len(leads_to_enrich),
            "errors": errors if errors else None
        }
        
    except Exception as e:
        session.rollback()
        logger.error(f"‚ùå [IMPORT] Erro ao processar CSV: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Erro ao processar arquivo CSV: {str(e)}"
        )


@router.post("/parse-linkedin-pdf")
async def parse_linkedin_pdf(
    file: UploadFile = File(...),
    session: Session = Depends(get_session),
    current_user: User = Depends(get_current_active_user)
):
    """
    Processa um PDF de exporta√ß√£o do LinkedIn e extrai informa√ß√µes usando IA
    
    Args:
        file: Arquivo PDF do LinkedIn
        
    Returns:
        JSON com dados extra√≠dos do LinkedIn
    """
    # Validar tipo de arquivo
    if not file.filename or not file.filename.lower().endswith('.pdf'):
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Arquivo deve ser um PDF (.pdf)"
        )
    
    # Validar tamanho m√°ximo (10MB)
    MAX_FILE_SIZE = 10 * 1024 * 1024  # 10MB
    file_content = await file.read()
    if len(file_content) > MAX_FILE_SIZE:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Arquivo muito grande. Tamanho m√°ximo: 10MB"
        )
    
    # Resetar posi√ß√£o do arquivo para leitura
    await file.seek(0)
    
    try:
        logger.info(f"üìÑ [PDF PARSER] Processando PDF: {file.filename}")
        
        # Verificar se LLM est√° dispon√≠vel antes de processar
        from app.agents.llm_helper import is_llm_available
        if not is_llm_available():
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                detail="LLM n√£o est√° dispon√≠vel. Verifique se o Ollama est√° rodando ou configure a API key do OpenAI no arquivo .env"
            )
        
        # Extrair texto do PDF
        text = await extract_text_from_pdf(file)
        
        if not text or len(text.strip()) < 100:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="PDF n√£o cont√©m texto suficiente para an√°lise. Pode ser um PDF escaneado (imagem)."
            )
        
        # Analisar texto com LLM
        try:
            parsed_data = await parse_linkedin_data_with_llm(
                text, 
                session=session, 
                tenant_id=current_user.tenant_id, 
                user_id=current_user.id
            )
        except ValueError as llm_error:
            error_msg = str(llm_error)
            # Melhorar mensagem de erro para conex√£o recusada
            if "Connection refused" in error_msg or "111" in error_msg:
                raise HTTPException(
                    status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                    detail="N√£o foi poss√≠vel conectar ao Ollama. Verifique se o Ollama est√° rodando. Em ambiente Docker, use 'host.docker.internal:11434' ou configure a URL correta no .env (OLLAMA_BASE_URL)"
                )
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"Erro ao processar PDF com LLM: {error_msg}"
            )
        
        logger.info(f"‚úÖ [PDF PARSER] Dados extra√≠dos com sucesso: {len(parsed_data)} campos")
        
        return {
            "success": True,
            "data": parsed_data,
            "message": "PDF processado com sucesso"
        }
        
    except HTTPException:
        raise
    except ValueError as e:
        error_msg = str(e)
        logger.error(f"‚ùå [PDF PARSER] Erro de valida√ß√£o: {error_msg}")
        # Verificar se √© erro de conex√£o
        if "Connection refused" in error_msg or "111" in error_msg or "conex√£o" in error_msg.lower():
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                detail="N√£o foi poss√≠vel conectar ao Ollama. Verifique se o Ollama est√° rodando. Em ambiente Docker, configure OLLAMA_BASE_URL no .env (ex: http://host.docker.internal:11434)"
            )
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=error_msg
        )
    except Exception as e:
        error_msg = str(e)
        logger.error(f"‚ùå [PDF PARSER] Erro ao processar PDF: {error_msg}")
        import traceback
        traceback.print_exc()
        
        # Mensagens de erro mais espec√≠ficas
        if "Connection refused" in error_msg or "111" in error_msg:
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                detail="N√£o foi poss√≠vel conectar ao Ollama. Verifique se o Ollama est√° rodando. Em ambiente Docker, configure OLLAMA_BASE_URL no .env (ex: http://host.docker.internal:11434)"
            )
        elif "LLM n√£o est√° configurado" in error_msg or "LLM n√£o est√° dispon√≠vel" in error_msg:
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                detail="LLM n√£o est√° configurado. Configure OpenAI (OPENAI_API_KEY) ou Ollama (OLLAMA_BASE_URL) no arquivo .env"
            )
        else:
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"Erro ao processar PDF: {error_msg}"
            )


@router.post("/import-from-linkedin-pdf", response_model=LeadResponse)
async def import_lead_from_linkedin_pdf(
    file: UploadFile = File(...),
    session: Session = Depends(get_session),
    current_user: User = Depends(get_current_active_user)
):
    """
    Importa um PDF de exporta√ß√£o do LinkedIn e cria um lead automaticamente
    
    Args:
        file: Arquivo PDF do LinkedIn
        
    Returns:
        Lead criado com os dados extra√≠dos do PDF
    """
    # Validar tipo de arquivo
    if not file.filename or not file.filename.lower().endswith('.pdf'):
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Arquivo deve ser um PDF (.pdf)"
        )
    
    # Validar tamanho m√°ximo (10MB)
    MAX_FILE_SIZE = 10 * 1024 * 1024  # 10MB
    file_content = await file.read()
    if len(file_content) > MAX_FILE_SIZE:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Arquivo muito grande. Tamanho m√°ximo: 10MB"
        )
    
    # Resetar posi√ß√£o do arquivo para leitura
    await file.seek(0)
    
    try:
        logger.info(f"üìÑ [PDF IMPORT] Importando lead de PDF: {file.filename}")
        
        # Verificar limite antes de processar
        await check_limit("leads", session, current_user)
        
        # Verificar se LLM est√° dispon√≠vel
        from app.agents.llm_helper import is_llm_available
        if not is_llm_available():
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                detail="LLM n√£o est√° dispon√≠vel. Verifique se o Ollama est√° rodando ou configure a API key do OpenAI no arquivo .env"
            )
        
        # Extrair texto do PDF
        text = await extract_text_from_pdf(file)
        
        if not text or len(text.strip()) < 100:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="PDF n√£o cont√©m texto suficiente para an√°lise. Pode ser um PDF escaneado (imagem)."
            )
        
        # Analisar texto com LLM
        try:
            parsed_data = await parse_linkedin_data_with_llm(
                text, 
                session=session, 
                tenant_id=current_user.tenant_id, 
                user_id=current_user.id
            )
        except ValueError as llm_error:
            error_msg = str(llm_error)
            if "Connection refused" in error_msg or "111" in error_msg:
                raise HTTPException(
                    status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                    detail="N√£o foi poss√≠vel conectar ao Ollama. Verifique se o Ollama est√° rodando. Em ambiente Docker, use 'host.docker.internal:11434' ou configure a URL correta no .env (OLLAMA_BASE_URL)"
                )
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"Erro ao processar PDF com LLM: {error_msg}"
            )
        
        logger.info(f"‚úÖ [PDF IMPORT] Dados extra√≠dos com sucesso: {list(parsed_data.keys())}")
        logger.info(f"üìã [PDF IMPORT] Dados extra√≠dos - Nome: {parsed_data.get('name')}, Empresa: {parsed_data.get('company')}, Cargo: {parsed_data.get('position')}, Segmento: {parsed_data.get('industry')}")
        
        # Validar que temos pelo menos um nome
        if not parsed_data.get("name"):
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="N√£o foi poss√≠vel extrair o nome do perfil do PDF. Verifique se o PDF cont√©m informa√ß√µes do perfil do LinkedIn."
            )
        
        # Se company ou position n√£o foram extra√≠dos diretamente, tentar extrair da experi√™ncia mais recente
        company = parsed_data.get("company")
        position = parsed_data.get("position")
        industry = parsed_data.get("industry")
        
        # Se n√£o tiver company ou position, tentar extrair da experi√™ncia mais recente
        if not company or not position:
            try:
                experience_json = parsed_data.get("linkedin_experience_json")
                if experience_json:
                    # Se for string JSON, parsear
                    if isinstance(experience_json, str):
                        experiences = json.loads(experience_json)
                    else:
                        experiences = experience_json
                    
                    if experiences and len(experiences) > 0:
                        # Pegar a experi√™ncia mais recente (primeira da lista geralmente √© a atual)
                        latest_exp = experiences[0]
                        if not company and latest_exp.get("company"):
                            company = latest_exp.get("company")
                            logger.info(f"üìã [PDF IMPORT] Empresa extra√≠da da experi√™ncia: {company}")
                        if not position and latest_exp.get("position"):
                            position = latest_exp.get("position")
                            logger.info(f"üìã [PDF IMPORT] Cargo extra√≠do da experi√™ncia: {position}")
                        
                        # Se n√£o tiver industry e houver descri√ß√£o na experi√™ncia, tentar inferir
                        if not industry and latest_exp.get("description"):
                            # Tentar inferir industry da descri√ß√£o da experi√™ncia
                            description = latest_exp.get("description", "").lower()
                            industry_keywords = {
                                "tecnologia": ["software", "tech", "desenvolvimento", "programa√ß√£o", "it", "ti", "sistema"],
                                "sa√∫de": ["m√©dico", "hospital", "cl√≠nica", "sa√∫de", "enfermagem"],
                                "financeiro": ["banco", "financeiro", "investimento", "cr√©dito", "fintech"],
                                "educa√ß√£o": ["escola", "universidade", "educa√ß√£o", "ensino", "acad√™mico"],
                                "varejo": ["varejo", "loja", "com√©rcio", "retail"],
                                "consultoria": ["consultoria", "consultor", "advisory"],
                                "manufatura": ["manufatura", "produ√ß√£o", "industrial", "f√°brica"],
                            }
                            
                            for ind, keywords in industry_keywords.items():
                                if any(keyword in description for keyword in keywords):
                                    industry = ind.title()
                                    logger.info(f"üìã [PDF IMPORT] Segmento inferido da descri√ß√£o: {industry}")
                                    break
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è [PDF IMPORT] Erro ao extrair company/position da experi√™ncia: {e}")
        
        # Log final dos dados que ser√£o usados
        logger.info(f"üìã [PDF IMPORT] Dados finais para cria√ß√£o - Nome: {parsed_data.get('name')}, Empresa: {company}, Cargo: {position}, Segmento: {industry}")
        
        # Preparar dados para criar o lead
        lead_data = LeadCreate(
            name=parsed_data.get("name"),
            email=parsed_data.get("email"),
            phone=parsed_data.get("phone"),
            company=company,
            position=position,
            industry=industry,
            website=parsed_data.get("website"),
            linkedin_url=parsed_data.get("linkedin_url"),
            linkedin_headline=parsed_data.get("linkedin_headline"),
            linkedin_about=parsed_data.get("linkedin_about"),
            linkedin_experience_json=parsed_data.get("linkedin_experience_json"),
            linkedin_education_json=parsed_data.get("linkedin_education_json"),
            linkedin_certifications_json=parsed_data.get("linkedin_certifications_json"),
            linkedin_skills=parsed_data.get("linkedin_skills"),
            linkedin_articles_json=parsed_data.get("linkedin_articles_json"),
            linkedin_connections_count=parsed_data.get("linkedin_connections_count"),
            linkedin_followers_count=parsed_data.get("linkedin_followers_count"),
            source="linkedin",
            status=LeadStatus.NEW
        )
        
        # Criar lead usando a fun√ß√£o existente (reutilizando c√≥digo)
        lead_dict = lead_data.dict()
        lead_dict = ensure_ownership(lead_dict, current_user)
        
        # Se assigned_to foi fornecido mas owner_id n√£o, usar assigned_to como owner_id
        if lead_dict.get("assigned_to") and not lead_dict.get("owner_id"):
            lead_dict["owner_id"] = lead_dict["assigned_to"]
        
        lead = Lead(
            **lead_dict,
            tenant_id=current_user.tenant_id
        )
        session.add(lead)
        session.commit()
        session.refresh(lead)
        
        # Calcular score automaticamente
        try:
            lead.score = calculate_lead_score(lead, session)
            session.add(lead)
            session.commit()
            session.refresh(lead)
        except Exception as score_error:
            logger.warning(f"‚ö†Ô∏è [SCORING] Erro ao calcular score na cria√ß√£o do lead {lead.id}: {score_error}")
        
        # Calcular ICP score automaticamente
        try:
            lead.icp_score = calculate_icp_score(lead)
            session.add(lead)
            session.commit()
            session.refresh(lead)
        except Exception as icp_error:
            logger.warning(f"‚ö†Ô∏è [ICP SCORING] Erro ao calcular ICP score na cria√ß√£o do lead {lead.id}: {icp_error}")
        
        # Track KPI activity for lead creation
        try:
            completed_goals = track_kpi_activity(
                session=session,
                user_id=current_user.id,
                tenant_id=current_user.tenant_id,
                metric_type=GoalMetricType.LEADS_CREATED,
                value=1.0,
                entity_type='Lead',
                entity_id=lead.id
            )
            session.commit()
            if completed_goals:
                logger.info(f"üéØ [KPI] {len(completed_goals)} goal(s) completed by lead creation")
        except Exception as kpi_error:
            logger.warning(f"‚ö†Ô∏è [KPI] Error tracking activity: {kpi_error}")
        
        # Track KPI activity for LinkedIn import
        try:
            completed_goals_linkedin = track_kpi_activity(
                session=session,
                user_id=lead.owner_id or current_user.id,
                tenant_id=current_user.tenant_id,
                metric_type=GoalMetricType.LEADS_IMPORTED_FROM_LINKEDIN,
                value=1.0,
                entity_type='Lead',
                entity_id=lead.id
            )
            session.commit()
            if completed_goals_linkedin:
                logger.info(f"üéØ [KPI] {len(completed_goals_linkedin)} goal(s) completed by LinkedIn import")
        except Exception as kpi_error:
            logger.warning(f"‚ö†Ô∏è [KPI] Erro ao trackear importa√ß√£o do LinkedIn: {kpi_error}")
            # N√£o falhar a opera√ß√£o principal se o tracking falhar
        
        logger.info(f"‚úÖ [PDF IMPORT] Lead criado com sucesso: ID {lead.id}, Nome: {lead.name}")
        
        return lead
        
    except HTTPException:
        raise
    except ValueError as e:
        error_msg = str(e)
        logger.error(f"‚ùå [PDF IMPORT] Erro de valida√ß√£o: {error_msg}")
        if "Connection refused" in error_msg or "111" in error_msg or "conex√£o" in error_msg.lower():
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                detail="N√£o foi poss√≠vel conectar ao Ollama. Verifique se o Ollama est√° rodando. Em ambiente Docker, configure OLLAMA_BASE_URL no .env (ex: http://host.docker.internal:11434)"
            )
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=error_msg
        )
    except Exception as e:
        error_msg = str(e)
        logger.error(f"‚ùå [PDF IMPORT] Erro ao importar lead de PDF: {error_msg}")
        import traceback
        traceback.print_exc()
        
        if "Connection refused" in error_msg or "111" in error_msg:
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                detail="N√£o foi poss√≠vel conectar ao Ollama. Verifique se o Ollama est√° rodando. Em ambiente Docker, configure OLLAMA_BASE_URL no .env (ex: http://host.docker.internal:11434)"
            )
        elif "LLM n√£o est√° configurado" in error_msg or "LLM n√£o est√° dispon√≠vel" in error_msg:
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                detail="LLM n√£o est√° configurado. Configure OpenAI (OPENAI_API_KEY) ou Ollama (OLLAMA_BASE_URL) no arquivo .env"
            )
        else:
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"Erro ao importar lead de PDF: {error_msg}"
            )


@router.get("/{lead_id}", response_model=LeadResponse)
async def get_lead(
    lead_id: int,
    session: Session = Depends(get_session),
    current_user: User = Depends(get_current_active_user)
):
    """Get a specific lead"""
    lead = session.get(Lead, lead_id)
    if not lead:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Lead not found"
        )
    require_ownership(lead, current_user)
    return lead


@router.put("/{lead_id}", response_model=LeadResponse)
async def update_lead(
    lead_id: int,
    lead_data: LeadCreate,
    session: Session = Depends(get_session),
    current_user: User = Depends(get_current_active_user)
):
    """Update a lead"""
    lead = session.get(Lead, lead_id)
    if not lead:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Lead not found"
        )
    require_ownership(lead, current_user)
    
    # Salvar estado antigo para verificar mudan√ßas relevantes
    # Criar um objeto tempor√°rio com os valores atuais
    old_status = lead.status
    old_values = {
        'email': lead.email,
        'phone': lead.phone,
        'linkedin_url': lead.linkedin_url,
        'linkedin_headline': lead.linkedin_headline,
        'linkedin_about': lead.linkedin_about,
        'linkedin_experience_json': lead.linkedin_experience_json,
        'linkedin_education_json': lead.linkedin_education_json,
        'cnpj': lead.cnpj,
        'situacao_cadastral': lead.situacao_cadastral,
        'capital_social': lead.capital_social,
        'porte': lead.porte,
        'industry': lead.industry,
        'company_size': lead.company_size,
        'tech_stack': lead.tech_stack,
        'is_hiring': lead.is_hiring,
        'is_advertising': lead.is_advertising,
        'status': lead.status,
        'last_contact': lead.last_contact,
        'next_followup': lead.next_followup,
        'owner_id': lead.owner_id
    }
    
    # Atualizar campos, mas manter ownership e tenant
    lead_dict = lead_data.dict()
    # Normalizar strings vazias para None em campos opcionais
    optional_string_fields = [
        'email', 'phone', 'company', 'position', 'website', 'linkedin_url',
        'linkedin_headline', 'linkedin_about', 'linkedin_experience_json',
        'linkedin_education_json', 'linkedin_certifications_json', 'linkedin_skills',
        'linkedin_articles_json', 'linkedin_recent_activity', 'linkedin_summary',
        'source', 'notes', 'tags', 'address', 'city', 'state', 'zip_code',
        'country', 'industry', 'company_size', 'context', 'tech_stack',
        'razao_social', 'nome_fantasia', 'cnpj', 'situacao_cadastral',
        'motivo_situacao_cadastral', 'natureza_juridica', 'porte', 'logradouro',
        'numero', 'bairro', 'cep', 'municipio', 'uf', 'complemento',
        'cnae_principal_codigo', 'cnae_principal_descricao', 'cnaes_secundarios_json',
        'telefone_empresa', 'email_empresa', 'socios_json', 'agent_suggestion'
    ]
    for key, value in lead_dict.items():
        if key not in ['owner_id', 'created_by_id', 'tenant_id']:
            # Converter strings vazias em None para campos opcionais
            if key in optional_string_fields and isinstance(value, str) and value.strip() == '':
                value = None
            setattr(lead, key, value)
    
    # Se owner_id foi especificado, atualizar (mas validar acesso)
    if lead_dict.get("owner_id") is not None:
        new_owner_id = lead_dict["owner_id"]
        # Verificar se o usu√°rio existe e pertence ao mesmo tenant
        if new_owner_id:
            user = session.get(User, new_owner_id)
            if not user or user.tenant_id != current_user.tenant_id:
                raise HTTPException(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    detail="Invalid user"
                )
        # Admin pode atribuir a qualquer usu√°rio do tenant ou remover (None)
        if current_user.role.value == "admin":
            lead.owner_id = new_owner_id
        # Usu√°rio normal s√≥ pode atribuir a si mesmo
        elif new_owner_id == current_user.id:
            lead.owner_id = new_owner_id
        elif new_owner_id is None:
            # N√£o permitir remover owner (sempre deve ter um)
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Owner is required. You can reassign to another user."
            )
        else:
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail="You can only assign leads to yourself"
            )
    
    session.add(lead)
    session.commit()
    session.refresh(lead)
    
    # Recalcular score se houver mudan√ßas relevantes
    # Criar objeto tempor√°rio com valores antigos para compara√ß√£o
    class OldLead:
        def __init__(self, values):
            for key, value in values.items():
                setattr(self, key, value)
    
    old_lead_obj = OldLead(old_values)
    if should_recalculate_score(old_lead_obj, lead):
        try:
            lead.score = calculate_lead_score(lead, session)
            session.add(lead)
            session.commit()
            session.refresh(lead)
        except Exception as score_error:
            logger.warning(f"‚ö†Ô∏è [SCORING] Erro ao recalcular score do lead {lead.id}: {score_error}")
            # N√£o falhar a opera√ß√£o principal se o scoring falhar
    
    # Recalcular ICP score se houver mudan√ßas nos campos ICP
    if should_recalculate_icp_score(old_lead_obj, lead):
        try:
            lead.icp_score = calculate_icp_score(lead)
            session.add(lead)
            session.commit()
            session.refresh(lead)
        except Exception as icp_error:
            logger.warning(f"‚ö†Ô∏è [ICP SCORING] Erro ao recalcular ICP score do lead {lead.id}: {icp_error}")
            # N√£o falhar a opera√ß√£o principal se o ICP scoring falhar
    
    # Track KPI activity if lead status changed to NURTURING
    if lead.status == LeadStatus.NURTURING and old_status != LeadStatus.NURTURING:
        try:
            completed_goals = track_kpi_activity(
                session=session,
                user_id=lead.owner_id or current_user.id,
                tenant_id=current_user.tenant_id,
                metric_type=GoalMetricType.LEADS_ENRICHED,
                value=1.0,
                entity_type='Lead',
                entity_id=lead.id
            )
            session.commit()
            if completed_goals:
                logger.info(f"üéØ [KPI] {len(completed_goals)} goal(s) completed by lead enrichment")
        except Exception as kpi_error:
            logger.warning(f"‚ö†Ô∏è [KPI] Erro ao trackear enriquecimento de lead: {kpi_error}")
            # N√£o falhar a opera√ß√£o principal se o tracking falhar
    
    return lead


@router.delete("/{lead_id}")
async def delete_lead(
    lead_id: int,
    session: Session = Depends(get_session),
    current_user: User = Depends(get_current_active_user)
):
    """Delete a lead"""
    lead = session.get(Lead, lead_id)
    if not lead:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Lead not found"
        )
    require_ownership(lead, current_user)
    
    session.delete(lead)
    session.commit()
    return {"message": "Lead deleted successfully"}


@router.post("/{lead_id}/comments", response_model=LeadCommentResponse)
async def create_lead_comment(
    lead_id: int,
    comment_data: LeadCommentCreate,
    session: Session = Depends(get_session),
    current_user: User = Depends(get_current_active_user)
):
    """Create a comment on a lead"""
    # Verify lead access
    lead = session.get(Lead, lead_id)
    if not lead:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Lead not found"
        )
    require_ownership(lead, current_user)
    
    # Create comment
    comment = LeadComment(
        tenant_id=current_user.tenant_id,
        lead_id=lead_id,
        user_id=current_user.id,
        comment=comment_data.comment
    )
    session.add(comment)
    session.commit()
    session.refresh(comment)
    
    # Update lead's updated_at
    lead.updated_at = datetime.utcnow()
    session.add(lead)
    session.commit()
    
    # Recalcular score ap√≥s adicionar coment√°rio
    try:
        lead.score = calculate_lead_score(lead, session)
        session.add(lead)
        session.commit()
        session.refresh(lead)
    except Exception as score_error:
        logger.warning(f"‚ö†Ô∏è [SCORING] Erro ao recalcular score do lead {lead.id} ap√≥s coment√°rio: {score_error}")
        # N√£o falhar a opera√ß√£o principal se o scoring falhar
    
    # Get user info for response
    user = session.get(User, current_user.id)
    response = LeadCommentResponse(
        id=comment.id,
        tenant_id=comment.tenant_id,
        lead_id=comment.lead_id,
        user_id=comment.user_id,
        comment=comment.comment,
        created_at=comment.created_at,
        updated_at=comment.updated_at,
        user_name=user.full_name if user else None,
        user_email=user.email if user else None
    )
    
    return response


@router.get("/{lead_id}/comments", response_model=List[LeadCommentResponse])
async def get_lead_comments(
    lead_id: int,
    session: Session = Depends(get_session),
    current_user: User = Depends(get_current_active_user)
):
    """Get all comments for a lead"""
    # Verify lead access
    lead = session.get(Lead, lead_id)
    if not lead:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Lead not found"
        )
    require_ownership(lead, current_user)
    
    # Get comments
    comments = session.exec(
        select(LeadComment)
        .where(
            and_(
                LeadComment.lead_id == lead_id,
                LeadComment.tenant_id == current_user.tenant_id
            )
        )
        .order_by(LeadComment.created_at.desc())
    ).all()
    
    # Get user info for each comment
    result = []
    for comment in comments:
        user = session.get(User, comment.user_id)
        result.append(LeadCommentResponse(
            id=comment.id,
            tenant_id=comment.tenant_id,
            lead_id=comment.lead_id,
            user_id=comment.user_id,
            comment=comment.comment,
            created_at=comment.created_at,
            updated_at=comment.updated_at,
            user_name=user.full_name if user else None,
            user_email=user.email if user else None
        ))
    
    return result


@router.delete("/comments/{comment_id}")
async def delete_lead_comment(
    comment_id: int,
    session: Session = Depends(get_session),
    current_user: User = Depends(get_current_active_user)
):
    """Delete a comment"""
    comment = session.get(LeadComment, comment_id)
    if not comment or comment.tenant_id != current_user.tenant_id:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Comment not found"
        )
    
    # Only allow deletion by comment owner or admin
    if comment.user_id != current_user.id and current_user.role != "admin":
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="You can only delete your own comments"
        )
    
    session.delete(comment)
    session.commit()
    return {"message": "Comment deleted successfully"}


@router.patch("/{lead_id}/status", response_model=LeadResponse)
async def update_lead_status(
    lead_id: int,
    new_status: LeadStatus = Query(..., description="New status for the lead"),
    session: Session = Depends(get_session),
    current_user: User = Depends(get_current_active_user)
):
    """Update lead status"""
    lead = session.get(Lead, lead_id)
    if not lead:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Lead not found"
        )
    require_ownership(lead, current_user)
    
    old_status = lead.status
    lead.status = new_status
    session.add(lead)
    session.commit()
    session.refresh(lead)
    
    # Track KPI activity if lead status changed to NURTURING
    if new_status == LeadStatus.NURTURING and old_status != LeadStatus.NURTURING:
        try:
            completed_goals = track_kpi_activity(
                session=session,
                user_id=lead.owner_id or current_user.id,
                tenant_id=current_user.tenant_id,
                metric_type=GoalMetricType.LEADS_ENRICHED,
                value=1.0,
                entity_type='Lead',
                entity_id=lead.id
            )
            session.commit()
            if completed_goals:
                logger.info(f"üéØ [KPI] {len(completed_goals)} goal(s) completed by lead enrichment")
        except Exception as kpi_error:
            logger.warning(f"‚ö†Ô∏è [KPI] Erro ao trackear enriquecimento de lead: {kpi_error}")
            # N√£o falhar a opera√ß√£o principal se o tracking falhar
    
    return lead


class BulkUpdateRequest(BaseModel):
    """Request para atualiza√ß√£o em massa de leads"""
    lead_ids: List[int] = Field(..., description="Lista de IDs dos leads a atualizar")
    field: str = Field(..., description="Nome do campo a atualizar")
    value: Optional[Any] = Field(None, description="Valor a ser definido (None para limpar o campo)")


@router.post("/bulk-update")
async def bulk_update_leads(
    update_data: BulkUpdateRequest,
    session: Session = Depends(get_session),
    current_user: User = Depends(get_current_active_user)
):
    """
    Atualiza m√∫ltiplos leads em massa
    Permite atualizar qualquer campo de uma lista de leads
    """
    if not update_data.lead_ids:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Nenhum lead especificado"
        )
    
    # Campos que n√£o podem ser atualizados em massa (por seguran√ßa)
    protected_fields = ['id', 'tenant_id', 'created_at', 'updated_at', 'created_by_id']
    
    if update_data.field in protected_fields:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Campo '{update_data.field}' n√£o pode ser atualizado em massa"
        )
    
    # Verificar se o campo existe no modelo Lead
    if not hasattr(Lead, update_data.field):
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Campo '{update_data.field}' n√£o existe no modelo Lead"
        )
    
    # Buscar os leads do tenant
    all_leads = session.exec(
        select(Lead).where(
            and_(
                Lead.tenant_id == current_user.tenant_id,
                Lead.id.in_(update_data.lead_ids)
            )
        )
    ).all()
    
    if len(all_leads) == 0:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Nenhum lead encontrado"
        )
    
    # Filtrar apenas os leads que o usu√°rio tem permiss√£o (owner ou admin)
    is_admin = current_user.role == UserRole.ADMIN or current_user.role.value == "admin"
    
    logger.info(f"üîê [BULK UPDATE] User {current_user.id} (role: {current_user.role}, is_admin: {is_admin}) tentando atualizar {len(all_leads)} leads")
    
    if is_admin:
        # Admin pode atualizar todos os leads do tenant
        leads = all_leads
        logger.info(f"‚úÖ [BULK UPDATE] Admin - permitindo atualiza√ß√£o de todos os {len(leads)} leads")
    else:
        # Usu√°rio normal s√≥ pode atualizar seus pr√≥prios leads (ou leads sem owner)
        leads = [lead for lead in all_leads if lead.owner_id == current_user.id or lead.owner_id is None]
        logger.info(f"üë§ [BULK UPDATE] Usu√°rio normal - permitindo atualiza√ß√£o de {len(leads)} leads (pr√≥prios ou sem owner)")
    
    if len(leads) == 0:
        logger.warning(f"‚ö†Ô∏è [BULK UPDATE] Nenhum lead pode ser atualizado. Total selecionado: {len(all_leads)}")
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Voc√™ n√£o tem permiss√£o para atualizar nenhum dos leads selecionados"
        )
    
    # Se alguns leads n√£o puderam ser atualizados, avisar mas continuar
    skipped_count = len(all_leads) - len(leads)
    
    # Converter valor se necess√°rio
    value = update_data.value
    
    # Tratamento especial para campos enum
    if update_data.field == 'status':
        try:
            value = LeadStatus(update_data.value) if update_data.value else None
        except ValueError:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"Status inv√°lido: {update_data.value}"
            )
    
    # Tratamento especial para campos num√©ricos
    if update_data.field in ['score', 'capital_social']:
        if value is not None:
            try:
                value = float(value) if update_data.field == 'capital_social' else int(value)
            except (ValueError, TypeError):
                raise HTTPException(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    detail=f"Valor inv√°lido para campo num√©rico: {update_data.value}"
                )
    
    # Tratamento especial para campos de data
    if update_data.field in ['last_contact', 'next_followup', 'data_abertura', 'data_situacao_cadastral', 
                             'data_opcao_simples', 'data_exclusao_simples']:
        if value:
            try:
                if isinstance(value, str):
                    value = datetime.fromisoformat(value.replace('Z', '+00:00'))
                elif isinstance(value, datetime):
                    pass  # J√° √© datetime
            except (ValueError, TypeError):
                raise HTTPException(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    detail=f"Formato de data inv√°lido: {update_data.value}"
                )
    
    # Tratamento especial para campos booleanos
    if update_data.field == 'simples_nacional':
        if value is not None:
            value = bool(value)
    
    # Atualizar todos os leads
    updated_count = 0
    for lead in leads:
        try:
            setattr(lead, update_data.field, value)
            lead.updated_at = datetime.utcnow()
            session.add(lead)
            updated_count += 1
        except Exception as e:
            logger.error(f"Erro ao atualizar lead {lead.id}: {e}")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"Erro ao atualizar lead {lead.id}: {str(e)}"
            )
    
    session.commit()
    
    # Recalcular score para todos os leads atualizados
    # (sempre recalcular em bulk update, pois pode ter mudado campos relevantes)
    for lead in leads:
        try:
            session.refresh(lead)
            lead.score = calculate_lead_score(lead, session)
            session.add(lead)
        except Exception as score_error:
            logger.warning(f"‚ö†Ô∏è [SCORING] Erro ao recalcular score do lead {lead.id} em bulk update: {score_error}")
            # N√£o falhar a opera√ß√£o principal se o scoring falhar
    
    session.commit()
    
    message = f"{updated_count} lead(s) atualizado(s) com sucesso"
    if skipped_count > 0:
        message += f". {skipped_count} lead(s) n√£o puderam ser atualizados (sem permiss√£o)"
    
    return {
        "success": True,
        "message": message,
        "updated_count": updated_count,
        "skipped_count": skipped_count
    }


@router.patch("/{lead_id}/assign", response_model=LeadResponse)
async def assign_lead(
    lead_id: int,
    user_id: Optional[int] = None,
    session: Session = Depends(get_session),
    current_user: User = Depends(get_current_active_user)
):
    """Assign or unassign a lead to a user"""
    lead = session.get(Lead, lead_id)
    if not lead:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Lead not found"
        )
    require_ownership(lead, current_user)
    
    if user_id:
        # Verify user belongs to same tenant
        user = session.get(User, user_id)
        if not user or user.tenant_id != current_user.tenant_id:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Invalid user"
            )
        # Admin pode atribuir a qualquer usu√°rio, usu√°rio normal s√≥ a si mesmo
        if current_user.role.value == "admin" or user_id == current_user.id:
            lead.owner_id = user_id
            lead.assigned_to = user_id  # Manter compatibilidade
        else:
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail="You can only assign leads to yourself"
            )
    else:
        # N√£o permitir remover owner (sempre deve ter um)
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Owner is required. You can reassign to another user."
        )
    
    session.add(lead)
    session.commit()
    session.refresh(lead)
    return lead


@router.post("/{lead_id}/convert-to-account", response_model=dict)
async def convert_lead_to_account(
    lead_id: int,
    session: Session = Depends(get_session),
    current_user: User = Depends(get_current_active_user)
):
    """Convert a lead to an account"""
    lead = session.get(Lead, lead_id)
    if not lead:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Lead not found"
        )
    require_ownership(lead, current_user)
    
    # Verificar se j√° existe account com o mesmo CNPJ
    account = None
    if lead.cnpj:
        account = session.exec(
            select(Account).where(
                and_(
                    Account.tenant_id == current_user.tenant_id,
                    Account.cnpj == lead.cnpj
                )
            )
        ).first()
    
    if not account:
        # Criar nova account
        account_data = AccountCreate(
            name=lead.company or lead.name or lead.razao_social or "Empresa sem nome",
            website=lead.website,
            phone=lead.phone or lead.telefone_empresa,
            email=lead.email or lead.email_empresa,
            industry=lead.industry,
            company_size=lead.company_size,
            address=lead.address or (f"{lead.logradouro or ''} {lead.numero or ''}".strip() if lead.logradouro else None),
            city=lead.city or lead.municipio,
            state=lead.state or lead.uf,
            zip_code=lead.zip_code or lead.cep,
            country=lead.country or "Brasil",
            description=lead.context or lead.notes,
            cnpj=lead.cnpj,
            razao_social=lead.razao_social,
            nome_fantasia=lead.nome_fantasia,
            owner_id=lead.owner_id
        )
        
        acc_dict = account_data.dict()
        acc_dict = ensure_ownership(acc_dict, current_user)
        
        account = Account(
            **acc_dict,
            tenant_id=current_user.tenant_id
        )
        session.add(account)
        session.commit()
        session.refresh(account)
    
    # Atualizar lead com account_id
    lead.account_id = account.id
    session.add(lead)
    session.commit()
    
    # Registrar auditoria
    log_convert(session, current_user, "Lead", lead_id, "Account", account.id)
    
    return {
        "message": "Lead converted to account successfully",
        "account_id": account.id,
        "account": account
    }


@router.post("/{lead_id}/convert-to-opportunity", response_model=dict)
async def convert_lead_to_opportunity(
    lead_id: int,
    stage_id: Optional[int] = Query(None),
    amount: Optional[float] = Query(None),
    session: Session = Depends(get_session),
    current_user: User = Depends(get_current_active_user)
):
    """Convert a lead to an opportunity"""
    lead = session.get(Lead, lead_id)
    if not lead:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Lead not found"
        )
    require_ownership(lead, current_user)
    
    # Verificar se lead tem account, se n√£o, criar
    account = None
    if lead.account_id:
        account = session.get(Account, lead.account_id)
        require_ownership(account, current_user)
    else:
        # Converter para account primeiro
        convert_result = await convert_lead_to_account(lead_id, session, current_user)
        account = convert_result["account"]
    
    # Buscar funil padr√£o e primeiro est√°gio se stage_id n√£o fornecido
    if not stage_id:
        from app.models import SalesFunnel
        default_funnel = session.exec(
            select(SalesFunnel).where(
                and_(
                    SalesFunnel.tenant_id == current_user.tenant_id,
                    SalesFunnel.is_default == True
                )
            )
        ).first()
        
        if not default_funnel:
            # Buscar qualquer funil do tenant
            default_funnel = session.exec(
                select(SalesFunnel).where(SalesFunnel.tenant_id == current_user.tenant_id)
            ).first()
        
        if default_funnel:
            stages = session.exec(
                select(SalesStage).where(
                    SalesStage.funnel_id == default_funnel.id
                ).order_by(SalesStage.order)
            ).all()
            if stages:
                stage_id = stages[0].id
            else:
                raise HTTPException(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    detail="No sales stages found. Please create a sales funnel with stages first."
                )
        else:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="No sales funnel found. Please create a sales funnel first."
            )
    
    # Verificar se stage existe
    stage = session.get(SalesStage, stage_id)
    if not stage:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Sales stage not found"
        )
    
    # Criar opportunity
    opportunity_data = OpportunityCreate(
        account_id=account.id,
        contact_id=lead.contact_id,
        stage_id=stage_id,
        name=f"Oportunidade: {lead.name or lead.company or 'Sem nome'}",
        description=lead.context or lead.notes,
        amount=amount,
        currency="BRL",
        expected_close_date=None,
        probability=stage.probability,
        notes=f"Convertido do lead #{lead_id}"
    )
    
    opp_dict = opportunity_data.dict()
    opp_dict = ensure_ownership(opp_dict, current_user)
    
    opportunity = Opportunity(
        **opp_dict,
        tenant_id=current_user.tenant_id
    )
    session.add(opportunity)
    session.commit()
    session.refresh(opportunity)
    
    # Registrar auditoria
    log_convert(session, current_user, "Lead", lead_id, "Opportunity", opportunity.id)
    
    return {
        "message": "Lead converted to opportunity successfully",
        "opportunity_id": opportunity.id,
        "opportunity": opportunity
    }


@router.post("/{lead_id}/generate-insight")
async def generate_insight(
    lead_id: int,
    language: Optional[str] = Query("pt-BR", description="Idioma para gerar o insight (pt-BR ou en)"),
    session: Session = Depends(get_session),
    current_user: User = Depends(get_current_active_user)
):
    """
    Gera um insight sobre o lead usando IA
    
    Args:
        lead_id: ID do lead
        
    Returns:
        JSON com o insight gerado
    """
    lead = session.get(Lead, lead_id)
    if not lead:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Lead not found"
        )
    require_ownership(lead, current_user)
    
    try:
        logger.info(f"ü§ñ [INSIGHT] Gerando insight para lead {lead_id} em idioma: {language}")
        insight = generate_lead_insight(lead, session, language=language)
        
        # Atualizar o campo linkedin_summary com o insight gerado
        lead.linkedin_summary = insight
        session.add(lead)
        session.commit()
        session.refresh(lead)
        
        logger.info(f"‚úÖ [INSIGHT] Insight gerado e salvo para lead {lead_id}")
        
        return {
            "success": True,
            "insight": insight,
            "message": "Insight gerado com sucesso"
        }
        
    except ValueError as e:
        error_msg = str(e)
        logger.error(f"‚ùå [INSIGHT] Erro ao gerar insight: {error_msg}")
        
        if "LLM n√£o est√° dispon√≠vel" in error_msg or "Connection refused" in error_msg:
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                detail="LLM n√£o est√° dispon√≠vel. Configure OpenAI ou Ollama no arquivo .env"
            )
        
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Erro ao gerar insight: {error_msg}"
        )
    except Exception as e:
        error_msg = str(e)
        logger.error(f"‚ùå [INSIGHT] Erro inesperado ao gerar insight: {error_msg}")
        import traceback
        traceback.print_exc()
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Erro ao gerar insight: {error_msg}"
        )

