import json
import re
import asyncio
import logging
import requests
from bs4 import BeautifulSoup
from typing import Dict, Any, Optional, List
from urllib.parse import urljoin
from app.agents.llm_helper import get_llm, is_llm_available
from app.config import settings
from datetime import datetime

# Configurar logger
logger = logging.getLogger(__name__)

# Importar googlesearch apenas se dispon√≠vel
try:
    from googlesearch import search as google_search
    GOOGLE_SEARCH_AVAILABLE = True
except ImportError:
    GOOGLE_SEARCH_AVAILABLE = False
    logger.warning("‚ö†Ô∏è googlesearch-python n√£o instalado. Google Search fallback n√£o dispon√≠vel.")

async def scrape_website(url: str) -> Dict[str, Any]:
    """Faz scraping do website do lead"""
    try:
        # Normalizar URL - adicionar https:// se n√£o tiver protocolo
        if not url.startswith(('http://', 'https://')):
            url = 'https://' + url
            logger.debug(f"üîß [RESEARCHER] URL normalizada para: {url}")
        
        # Headers mais completos para evitar bloqueio
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'pt-BR,pt;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0'
        }
        
        logger.info(f"üåê [RESEARCHER] Fazendo requisi√ß√£o para: {url}")
        
        # Adicionar timeout maior e verificar SSL
        response = requests.get(
            url, 
            headers=headers, 
            timeout=15,
            verify=True,
            allow_redirects=True
        )
        
        logger.info(f"üì° [RESEARCHER] Resposta recebida. Status: {response.status_code}")
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        logger.info(f"‚úÖ [RESEARCHER] HTML parseado com sucesso. Tamanho: {len(response.content)} bytes")
        
        # Extrair informa√ß√µes b√°sicas
        title = soup.find('title')
        meta_description = soup.find('meta', attrs={'name': 'description'}) or soup.find('meta', attrs={'property': 'og:description'})
        
        # Extrair textos principais
        texts = []
        for tag in ['h1', 'h2', 'h3', 'p']:
            elements = soup.find_all(tag)
            texts.extend([elem.get_text(strip=True) for elem in elements if elem.get_text(strip=True)])
        
        logger.info(f"üìù [RESEARCHER] Extra√≠dos {len(texts)} textos do site")
        
        # Extrair links importantes
        links = []
        for link in soup.find_all('a', href=True)[:20]:
            href = link.get('href')
            text = link.get_text(strip=True)
            if href and text:
                # Converter URLs relativas para absolutas
                if href.startswith('/'):
                    href = urljoin(url, href)
                links.append({'text': text, 'url': href})
        
        result = {
            'title': title.get_text(strip=True) if title else None,
            'description': meta_description.get('content') if meta_description else None,
            'main_texts': texts[:50],  # Limitar quantidade
            'important_links': links,
            'url': url,
            'soup': soup  # Retornar soup para extra√ß√£o de contatos
        }
        
        logger.info(f"‚úÖ [RESEARCHER] Scraping conclu√≠do com sucesso!")
        return result
    except requests.exceptions.HTTPError as e:
        status_code = None
        if hasattr(e, 'response') and e.response is not None:
            status_code = e.response.status_code
        
        logger.error(f"‚ùå [RESEARCHER] Erro HTTP ao acessar {url}: Status {status_code}, Erro: {str(e)}")
        
        if status_code == 403:
            return {
                'success': False,
                'error': 'Acesso negado (403 Forbidden). O website possui prote√ß√£o anti-bot que impede acesso automatizado.',
                'url': url,
                'status_code': 403,
                'suggestion': 'O site bloqueou o acesso automatizado. Tente acessar manualmente para coletar informa√ß√µes ou considere usar uma ferramenta de scraping mais avan√ßada.'
            }
        return {
            'success': False,
            'error': f'Erro HTTP {status_code or "desconhecido"}: {str(e)}',
            'url': url,
            'status_code': status_code
        }
    except requests.exceptions.Timeout as e:
        logger.error(f"‚è±Ô∏è [RESEARCHER] Timeout ao acessar {url}: {str(e)}")
        return {'success': False, 'error': 'Timeout ao acessar o website. O servidor demorou muito para responder.', 'url': url}
    except requests.exceptions.ConnectionError as e:
        logger.error(f"üîå [RESEARCHER] Erro de conex√£o ao acessar {url}: {str(e)}")
        return {'success': False, 'error': f'Erro de conex√£o: {str(e)}', 'url': url}
    except requests.exceptions.SSLError as e:
        logger.error(f"üîí [RESEARCHER] Erro SSL ao acessar {url}: {str(e)}")
        return {'success': False, 'error': f'Erro SSL ao acessar o website: {str(e)}', 'url': url}
    except Exception as e:
        logger.error(f"‚ùå [RESEARCHER] Erro inesperado ao acessar {url}: {str(e)}")
        import traceback
        traceback.print_exc()
        return {'success': False, 'error': f'Erro inesperado: {str(e)}', 'url': url}


def extract_contact_info(soup: BeautifulSoup) -> Dict[str, Any]:
    """Extrai informa√ß√µes de contato do website"""
    contact_info = {
        'phone': None,
        'email': None,
        'address': None,
        'city': None,
        'state': None,
        'zip_code': None,
        'country': None
    }
    
    if not soup:
        return contact_info
    
    all_text = soup.get_text()
    
    # Extrair telefones (padr√µes brasileiros e internacionais)
    phone_patterns = [
        r'\(?\d{2}\)?\s?\d{4,5}[-.\s]?\d{4}',  # Brasil: (11) 98765-4321
        r'\+?\d{1,3}[-.\s]?\(?\d{1,4}\)?[-.\s]?\d{1,4}[-.\s]?\d{1,9}',  # Internacional
    ]
    
    for pattern in phone_patterns:
        matches = re.findall(pattern, all_text)
        if matches:
            phone = matches[0].strip()
            # Limpar formata√ß√£o mas manter n√∫meros e +
            phone_clean = re.sub(r'[^\d+]', '', phone)
            if len(phone_clean) >= 10:
                contact_info['phone'] = phone_clean
                break
    
    # Extrair emails
    email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
    emails = re.findall(email_pattern, all_text)
    if emails:
        # Filtrar emails gen√©ricos
        valid_emails = [e for e in emails if not any(x in e.lower() for x in ['example', 'test', 'noreply', 'no-reply'])]
        if valid_emails:
            contact_info['email'] = valid_emails[0]
    
    # Extrair endere√ßo (buscar em se√ß√µes espec√≠ficas)
    contact_sections = soup.find_all(['footer', 'div'], class_=re.compile(r'contact|address|footer', re.I))
    for section in contact_sections:
        section_text = section.get_text()
        
        # Tentar encontrar CEP brasileiro
        cep_match = re.search(r'\d{5}[-]?\d{3}', section_text)
        if cep_match:
            contact_info['zip_code'] = cep_match.group()
        
        # Tentar encontrar cidade e estado
        city_state_pattern = r'([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)\s*[-,\s]*\s*([A-Z]{2})'
        city_state_match = re.search(city_state_pattern, section_text)
        if city_state_match:
            contact_info['city'] = city_state_match.group(1)
            contact_info['state'] = city_state_match.group(2)
        
        # Tentar encontrar endere√ßo completo
        address_match = re.search(r'(?:Rua|Av|Avenida|Rodovia|Estrada|Pra√ßa|Alameda)[^,]+,\s*\d+[^,]*', section_text)
        if address_match:
            contact_info['address'] = address_match.group().strip()
    
    return contact_info


async def analyze_website_content(website_data: Dict[str, Any], lead_info: Dict[str, Any]) -> Dict[str, Any]:
    """Usa LLM para analisar o conte√∫do do site e extrair informa√ß√µes relevantes"""
    llm = get_llm(temperature=0.3)
    
    if not llm:
        # Fallback sem LLM
        return {
            'summary': 'An√°lise b√°sica do website realizada',
            'key_findings': website_data.get('main_texts', [])[:5],
            'recommendations': 'Configure LLM (OpenAI ou Ollama) no arquivo .env para an√°lise avan√ßada'
        }
    
    # Preparar conte√∫do para an√°lise
    content_text = '\n'.join(website_data.get('main_texts', [])[:100])
    
    prompt = f"""
    Voc√™ √© um pesquisador especializado em an√°lise de empresas para vendas B2B.
    
    Informa√ß√µes do Lead:
    - Nome: {lead_info.get('name', 'N/A')}
    - Empresa: {lead_info.get('company', 'N/A')}
    - Cargo: {lead_info.get('position', 'N/A')}
    
    Conte√∫do do Website da Empresa:
    {content_text}
    
    Analise o website e extraia informa√ß√µes estruturadas. Retorne APENAS um JSON v√°lido com a seguinte estrutura:
    {{
        "industry": "setor/ind√∫stria da empresa ou null",
        "company_size": "tamanho estimado (ex: '50-200 funcion√°rios', 'Startup', 'Grande empresa') ou null",
        "context": "resumo completo da empresa incluindo: o que fazem, principais produtos/servi√ßos, tecnologias utilizadas, dores/pain points identificados, oportunidades de vendas, e qualquer informa√ß√£o relevante para prospec√ß√£o. Seja detalhado mas objetivo.",
        "pain_points": ["dor 1", "dor 2", "..."],
        "opportunities": ["oportunidade 1", "oportunidade 2", "..."]
    }}
    
    IMPORTANTE:
    - O campo "context" deve ser um texto completo e detalhado (m√≠nimo 200 palavras) sobre a empresa
    - Se um campo n√£o for encontrado, retorne null
    - Seja preciso e extraia apenas informa√ß√µes claramente presentes no conte√∫do
    - O contexto deve ser √∫til para um SDR fazer uma abordagem personalizada
    """
    
    try:
        response = llm.invoke(prompt)
        # Tentar extrair JSON da resposta
        content = response.content
        
        # Se a resposta j√° √© JSON, parsear
        if content.strip().startswith('{'):
            return json.loads(content)
        else:
            # Se n√£o, criar estrutura b√°sica
            return {
                'summary': content[:500],
                'analysis': content,
                'extracted_data': {
                    'industry': 'A ser identificado',
                    'company_size': 'A ser identificado',
                    'products': 'A ser identificado'
                }
            }
    except Exception as e:
        return {
            'error': f'Erro na an√°lise: {str(e)}',
            'raw_content': content_text[:500]
        }


async def enrich_lead_data_with_llm(website_data: Dict[str, Any], contact_info: Dict[str, Any], lead_info: Dict[str, Any]) -> Dict[str, Any]:
    """Usa LLM para extrair e estruturar dados do lead"""
    llm = get_llm(temperature=0.2)  # Mais preciso para extra√ß√£o de dados
    
    if not llm:
        # Fallback sem LLM - retornar apenas dados de contato extra√≠dos
        return {
            **contact_info,
            'industry': None,
            'company_size': None,
            'context': 'An√°lise b√°sica realizada. Configure LLM (OpenAI ou Ollama) no arquivo .env para an√°lise avan√ßada.'
        }
    
    content_text = '\n'.join(website_data.get('main_texts', [])[:150])
    
    prompt = f"""
    Voc√™ √© um especialista em extra√ß√£o de dados de empresas a partir de websites.
    
    Informa√ß√µes j√° conhecidas do Lead:
    - Nome: {lead_info.get('name', 'N/A')}
    - Empresa: {lead_info.get('company', 'N/A')}
    - Cargo: {lead_info.get('position', 'N/A')}
    
    Conte√∫do do Website:
    {content_text}
    
    Informa√ß√µes de contato j√° extra√≠das:
    {json.dumps(contact_info, indent=2)}
    
    Analise o conte√∫do e extraia informa√ß√µes estruturadas. Retorne APENAS um JSON v√°lido com a seguinte estrutura:
    {{
        "phone": "telefone encontrado ou null",
        "email": "email encontrado ou null",
        "address": "endere√ßo completo ou null",
        "city": "cidade ou null",
        "state": "estado (sigla) ou null",
        "zip_code": "CEP ou null",
        "country": "pa√≠s ou 'Brasil'",
        "industry": "setor/ind√∫stria da empresa",
        "company_size": "tamanho estimado (ex: '50-200 funcion√°rios', 'Startup', 'Grande empresa')",
        "context": "resumo completo da empresa incluindo: o que fazem, principais produtos/servi√ßos, tecnologias utilizadas, dores/pain points identificados, oportunidades de vendas, e qualquer informa√ß√£o relevante para prospec√ß√£o. Seja detalhado mas objetivo (m√≠nimo 200 palavras)."
    }}
    
    IMPORTANTE:
    - Se um campo n√£o for encontrado, retorne null
    - Para telefone, use apenas n√∫meros e + se internacional
    - Para CEP brasileiro, use formato 12345-678
    - Para estado, use sigla (SP, RJ, MG, etc)
    - O campo "context" √© cr√≠tico e deve ser um texto completo e detalhado
    - Seja preciso e extraia apenas informa√ß√µes claramente presentes no conte√∫do
    """
    
    try:
        response = llm.invoke(prompt)
        content = response.content.strip()
        
        # Remover markdown code blocks se existirem
        if '```json' in content:
            content = content.split('```json')[1].split('```')[0]
        elif '```' in content:
            content = content.split('```')[1].split('```')[0]
        
        extracted = json.loads(content)
        
        # Mesclar com informa√ß√µes j√° extra√≠das (priorizar dados j√° encontrados)
        enriched_data = {**contact_info}
        for key, value in extracted.items():
            if value and (not enriched_data.get(key) or enriched_data.get(key) == 'null'):
                enriched_data[key] = value
        
        return enriched_data
    except Exception as e:
        logger.error(f"Erro ao enriquecer dados com LLM: {e}")
        return contact_info


async def enrich_via_google_search(company_name: str, domain: str, lead_info: Dict[str, Any]) -> Dict[str, Any]:
    """Enriquece lead usando busca no Google + LLM quando scraping direto falha"""
    if not GOOGLE_SEARCH_AVAILABLE:
        return {'success': False, 'error': 'Google Search n√£o dispon√≠vel (biblioteca n√£o instalada)'}
    
    if not is_llm_available():
        return {'success': False, 'error': 'LLM n√£o configurado. Configure OpenAI ou Ollama no arquivo .env'}
    
    logger.info(f"üîç [GOOGLE SEARCH] Buscando informa√ß√µes sobre: {company_name}")
    
    # Verificar assinatura da fun√ß√£o google_search para debug
    try:
        import inspect
        sig = inspect.signature(google_search)
        logger.debug(f"üìã [GOOGLE SEARCH] Assinatura da fun√ß√£o: {sig}")
    except Exception as e:
        logger.debug(f"‚ö†Ô∏è [GOOGLE SEARCH] N√£o foi poss√≠vel obter assinatura: {e}")
    
    try:
        # Buscar informa√ß√µes p√∫blicas sobre a empresa
        # Usar queries mais simples e diretas para aumentar chances de resultados
        search_queries = [
            company_name,  # Query mais simples primeiro
            f'{company_name} contato',
            f'{company_name} telefone endere√ßo',
            f'site:{domain}',
        ]
        
        search_results = []
        for i, query in enumerate(search_queries[:3], 1):  # Limitar para n√£o exceder rate limits
            try:
                logger.info(f"üîç [GOOGLE SEARCH] Buscando query {i}/3: {query}")
                # googlesearch-python: baseado na assinatura real: (term, num_results=10, lang='en', ...)
                # A biblioteca aceita: term, num_results, lang, mas N√ÉO aceita tld, stop, pause
                results = []
                try:
                    # Usar par√¢metros corretos: term (query), num_results, lang
                    # Aumentar num_results para 10 para ter mais chances de encontrar resultados
                    logger.info(f"üìã [GOOGLE SEARCH] Buscando: '{query}' (num_results=10, lang='pt')")
                    results = list(google_search(term=query, num_results=10, lang='pt'))
                    logger.info(f"‚úÖ [GOOGLE SEARCH] Query {i} retornou {len(results)} resultados")
                    if results:
                        logger.info(f"üìã [GOOGLE SEARCH] Primeiros resultados da query {i}: {results[:3]}")
                        search_results.extend(results[:5])  # Usar mais resultados por query
                    else:
                        logger.warning(f"‚ö†Ô∏è [GOOGLE SEARCH] Query {i} n√£o retornou resultados")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è [GOOGLE SEARCH] Erro na busca para query '{query}': {e}")
                    import traceback
                    traceback.print_exc()
                    continue
                await asyncio.sleep(1)  # Delay entre buscas
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è [GOOGLE SEARCH] Erro na busca Google para query '{query}': {e}")
                logger.warning(f"‚ö†Ô∏è [GOOGLE SEARCH] Tipo do erro: {type(e).__name__}")
                import traceback
                traceback.print_exc()
                continue
        
        # Remover duplicatas mantendo ordem
        seen = set()
        unique_results = []
        for url in search_results:
            if url not in seen:
                seen.add(url)
                unique_results.append(url)
        search_results = unique_results
        
        logger.info(f"üìä [GOOGLE SEARCH] Total de resultados √∫nicos: {len(search_results)}")
        if search_results:
            logger.info(f"üìã [GOOGLE SEARCH] Primeiros resultados: {search_results[:3]}")
        
        if not search_results:
            return {'success': False, 'error': f'Nenhum resultado encontrado no Google para "{company_name}". Tente verificar o nome da empresa ou usar APIs pagas como Hunter.io ou Clearbit.'}
        
        logger.info(f"‚úÖ [GOOGLE SEARCH] Encontrados {len(search_results)} resultados √∫nicos")
        
        # Usar LLM para analisar resultados e extrair informa√ß√µes
        llm = get_llm(temperature=0.2)
        if not llm:
            return {'success': False, 'error': 'LLM n√£o dispon√≠vel para an√°lise de resultados'}
        
        results_text = '\n'.join([f"- {url}" for url in search_results[:10]])
        
        prompt = f"""
        Voc√™ √© um especialista em pesquisa de empresas para vendas B2B.
        
        Informa√ß√µes conhecidas do Lead:
        - Nome: {lead_info.get('name', 'N/A')}
        - Empresa: {company_name}
        - Cargo: {lead_info.get('position', 'N/A')}
        - Dom√≠nio: {domain}
        
        Resultados de busca no Google sobre esta empresa:
        {results_text}
        
        Com base nos resultados de busca acima, extraia informa√ß√µes estruturadas sobre a empresa.
        Retorne APENAS um JSON v√°lido com a seguinte estrutura:
        {{
            "phone": "telefone encontrado ou null",
            "email": "email encontrado ou null",
            "address": "endere√ßo completo ou null",
            "city": "cidade ou null",
            "state": "estado (sigla) ou null",
            "zip_code": "CEP ou null",
            "country": "pa√≠s ou 'Brasil'",
            "industry": "setor/ind√∫stria da empresa",
            "company_size": "tamanho estimado (ex: '50-200 funcion√°rios', 'Startup', 'Grande empresa')",
            "context": "resumo completo da empresa incluindo: o que fazem, principais produtos/servi√ßos, tecnologias utilizadas, dores/pain points identificados, oportunidades de vendas, e qualquer informa√ß√£o relevante para prospec√ß√£o. Seja detalhado mas objetivo (m√≠nimo 200 palavras)."
        }}
        
        IMPORTANTE:
        - Se um campo n√£o for encontrado, retorne null
        - Para telefone, use apenas n√∫meros e + se internacional
        - Para CEP brasileiro, use formato 12345-678
        - Para estado, use sigla (SP, RJ, MG, etc)
        - O campo "context" √© cr√≠tico e deve ser um texto completo e detalhado
        - Seja preciso e extraia apenas informa√ß√µes que podem ser inferidas dos resultados de busca
        """
        
        logger.info(f"ü§ñ [GOOGLE SEARCH] Enviando {len(search_results)} resultados para an√°lise com LLM...")
        logger.info(f"üìù [GOOGLE SEARCH] Prompt length: {len(prompt)} caracteres")
        
        try:
            response = llm.invoke(prompt)
            logger.info(f"‚úÖ [GOOGLE SEARCH] Resposta do LLM recebida. Tamanho: {len(response.content)} caracteres")
            content = response.content.strip()
            
            # Remover markdown code blocks se existirem
            if '```json' in content:
                content = content.split('```json')[1].split('```')[0]
                logger.debug("üîß [GOOGLE SEARCH] Removido markdown code block (```json)")
            elif '```' in content:
                content = content.split('```')[1].split('```')[0]
                logger.debug("üîß [GOOGLE SEARCH] Removido markdown code block (```)")
            
            logger.info(f"üìã [GOOGLE SEARCH] Tentando fazer parse do JSON...")
            logger.debug(f"üìã [GOOGLE SEARCH] Primeiros 500 caracteres da resposta: {content[:500]}")
            
            extracted = json.loads(content)
            logger.info(f"‚úÖ [GOOGLE SEARCH] JSON parseado com sucesso! Campos extra√≠dos: {list(extracted.keys())}")
            
            return {
                'success': True,
                'enriched_data': extracted,
                'method': 'google_search',
                'sources': search_results[:5]
            }
        except json.JSONDecodeError as json_error:
            logger.error(f"‚ùå [GOOGLE SEARCH] Erro ao fazer parse do JSON retornado pelo LLM: {json_error}")
            logger.error(f"üìã [GOOGLE SEARCH] Conte√∫do que falhou: {content[:1000]}")
            return {'success': False, 'error': f'Erro ao processar resposta do LLM: JSON inv√°lido. O modelo pode ter retornado texto n√£o estruturado.'}
        except Exception as llm_error:
            logger.error(f"‚ùå [GOOGLE SEARCH] Erro ao invocar LLM: {llm_error}")
            import traceback
            traceback.print_exc()
            return {'success': False, 'error': f'Erro ao processar com LLM: {str(llm_error)}'}
        
    except Exception as e:
        logger.error(f"‚ùå [GOOGLE SEARCH] Erro geral: {e}")
        import traceback
        traceback.print_exc()
        return {'success': False, 'error': f'Erro na busca Google: {str(e)}'}


async def enrich_via_hunter_api(domain: str, company_name: str = None) -> Dict[str, Any]:
    """Enriquece lead usando Hunter.io API para buscar emails e informa√ß√µes por dom√≠nio"""
    if not settings.hunter_api_key:
        return {'success': False, 'error': 'Hunter.io API key n√£o configurada'}
    
    logger.info(f"üîç [HUNTER.IO] Buscando informa√ß√µes do dom√≠nio: {domain}")
    
    try:
        # Buscar informa√ß√µes do dom√≠nio
        url = f"https://api.hunter.io/v2/domain-search"
        params = {
            'domain': domain,
            'api_key': settings.hunter_api_key
        }
        
        response = requests.get(url, params=params, timeout=10)
        response.raise_for_status()
        
        data = response.json()
        
        if data.get('data'):
            domain_data = data['data']
            
            # Extrair emails encontrados
            emails = []
            if domain_data.get('emails'):
                emails = [email.get('value') for email in domain_data['emails'][:5]]
            
            # Extrair outras informa√ß√µes
            enriched_data = {
                'email': emails[0] if emails else None,
                'phone': domain_data.get('phone_numbers', [None])[0] if domain_data.get('phone_numbers') else None,
                'company_size': f"{domain_data.get('employees', 'N/A')} funcion√°rios" if domain_data.get('employees') else None,
                'industry': domain_data.get('industry') if domain_data.get('industry') else None,
                'country': domain_data.get('country') if domain_data.get('country') else 'Brasil',
                'context': f"Informa√ß√µes da empresa {company_name or domain}: "
            }
            
            # Adicionar contexto se houver descri√ß√£o
            if domain_data.get('description'):
                enriched_data['context'] += domain_data['description']
            
            logger.info(f"‚úÖ [HUNTER.IO] Encontradas informa√ß√µes: email={enriched_data.get('email')}, telefone={enriched_data.get('phone')}")
            
            return {
                'success': True,
                'enriched_data': enriched_data,
                'method': 'hunter_io',
                'emails_found': emails
            }
        else:
            return {'success': False, 'error': 'Nenhuma informa√ß√£o encontrada no Hunter.io'}
            
    except requests.exceptions.HTTPError as e:
        status_code = e.response.status_code if hasattr(e, 'response') and e.response else None
        error_msg = f'Erro HTTP {status_code} na API Hunter.io'
        if status_code == 401:
            error_msg = 'API key do Hunter.io inv√°lida ou expirada'
        elif status_code == 429:
            error_msg = 'Limite de requisi√ß√µes do Hunter.io excedido'
        return {'success': False, 'error': error_msg}
    except Exception as e:
        logger.error(f"‚ùå [HUNTER.IO] Erro: {e}")
        return {'success': False, 'error': f'Erro na API Hunter.io: {str(e)}'}


async def enrich_via_clearbit_api(domain: str) -> Dict[str, Any]:
    """Enriquece lead usando Clearbit API para buscar informa√ß√µes empresariais"""
    if not settings.clearbit_api_key:
        return {'success': False, 'error': 'Clearbit API key n√£o configurada'}
    
    logger.info(f"üîç [CLEARBIT] Buscando informa√ß√µes do dom√≠nio: {domain}")
    
    try:
        url = f"https://company.clearbit.com/v2/companies/find?domain={domain}"
        headers = {
            'Authorization': f'Bearer {settings.clearbit_api_key}'
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        data = response.json()
        
        enriched_data = {
            'phone': data.get('phone') if data.get('phone') else None,
            'address': data.get('geo', {}).get('streetNumber', '') + ' ' + data.get('geo', {}).get('streetName', '') if data.get('geo') else None,
            'city': data.get('geo', {}).get('city') if data.get('geo') else None,
            'state': data.get('geo', {}).get('state') if data.get('geo') else None,
            'zip_code': data.get('geo', {}).get('zip') if data.get('geo') else None,
            'country': data.get('geo', {}).get('country') if data.get('geo') else None,
            'industry': data.get('category', {}).get('industry') if data.get('category') else None,
            'company_size': f"{data.get('metrics', {}).get('employees', 'N/A')} funcion√°rios" if data.get('metrics', {}).get('employees') else None,
            'context': data.get('description') if data.get('description') else None
        }
        
        # Limpar valores None
        enriched_data = {k: v for k, v in enriched_data.items() if v}
        
        logger.info(f"‚úÖ [CLEARBIT] Encontradas informa√ß√µes: {len(enriched_data)} campos")
        
        return {
            'success': True,
            'enriched_data': enriched_data,
            'method': 'clearbit'
        }
        
    except requests.exceptions.HTTPError as e:
        status_code = e.response.status_code if hasattr(e, 'response') and e.response else None
        error_msg = f'Erro HTTP {status_code} na API Clearbit'
        if status_code == 401:
            error_msg = 'API key do Clearbit inv√°lida ou expirada'
        elif status_code == 404:
            error_msg = 'Dom√≠nio n√£o encontrado no Clearbit'
        elif status_code == 429:
            error_msg = 'Limite de requisi√ß√µes do Clearbit excedido'
        return {'success': False, 'error': error_msg}
    except Exception as e:
        logger.error(f"‚ùå [CLEARBIT] Erro: {e}")
        return {'success': False, 'error': f'Erro na API Clearbit: {str(e)}'}


async def enrich_via_serper_api(company_name: str, domain: str, lead_info: Dict[str, Any]) -> Dict[str, Any]:
    """Enriquece lead usando Serper.dev API (Google Search + Knowledge Graph)"""
    logger.info(f"üîç [SERPER] Fun√ß√£o chamada para: {company_name} (dom√≠nio: {domain})")
    logger.info(f"üîç [SERPER] API key presente: {settings.serper_api_key is not None}")
    
    if not settings.serper_api_key:
        logger.warning("‚ö†Ô∏è [SERPER] API key n√£o configurada")
        return {'success': False, 'error': 'Serper.dev API key n√£o configurada'}
    
    logger.info(f"üîç [SERPER] Buscando informa√ß√µes sobre: {company_name} (dom√≠nio: {domain})")
    
    try:
        enriched_data = {}
        sources = []
        
        # Usar Serper Search API que retorna knowledge graph automaticamente
        search_url = "https://google.serper.dev/search"
        search_payload = {
            "q": company_name,
            "gl": "br",  # Brasil
            "hl": "pt",  # Portugu√™s
            "num": 10
        }
        
        headers = {
            'X-API-KEY': settings.serper_api_key,
            'Content-Type': 'application/json'
        }
        
        try:
            logger.info(f"üìã [SERPER] Fazendo requisi√ß√£o POST para: {search_url}")
            logger.info(f"üìã [SERPER] Payload: {search_payload}")
            response = requests.post(search_url, json=search_payload, headers=headers, timeout=10)
            logger.info(f"üì° [SERPER] Resposta recebida. Status: {response.status_code}")
            response.raise_for_status()
            
            data = response.json()
            logger.info(f"‚úÖ [SERPER] JSON parseado com sucesso. Keys: {list(data.keys())}")
        except requests.exceptions.HTTPError as e:
            status_code = e.response.status_code if hasattr(e, 'response') and e.response else None
            error_msg = f'Erro HTTP {status_code} na API Serper'
            if status_code == 401:
                error_msg = 'API key do Serper inv√°lida ou expirada'
            elif status_code == 429:
                error_msg = 'Limite de requisi√ß√µes do Serper excedido'
            logger.error(f"‚ùå [SERPER] {error_msg}")
            return {'success': False, 'error': error_msg}
        
        # Extrair informa√ß√µes do Knowledge Graph se dispon√≠vel
        if 'knowledgeGraph' in data:
            kg = data['knowledgeGraph']
            logger.info(f"‚úÖ [SERPER] Knowledge Graph encontrado!")
            
            if kg.get('title'):
                logger.debug(f"üìã [SERPER] T√≠tulo: {kg.get('title')}")
            
            if kg.get('description'):
                enriched_data['context'] = kg.get('description')
            
            if kg.get('type'):
                enriched_data['industry'] = kg.get('type')
            
            # Extrair atributos do Knowledge Graph
            if 'attributes' in kg:
                attrs = kg['attributes']
                for attr in attrs:
                    if attr.get('label') == 'Telefone' or attr.get('label') == 'Phone':
                        enriched_data['phone'] = attr.get('value')
                    elif attr.get('label') == 'Endere√ßo' or attr.get('label') == 'Address':
                        enriched_data['address'] = attr.get('value')
                    elif attr.get('label') == 'Cidade' or attr.get('label') == 'City':
                        enriched_data['city'] = attr.get('value')
                    elif attr.get('label') == 'Estado' or attr.get('label') == 'State':
                        enriched_data['state'] = attr.get('value')
        
        # Extrair informa√ß√µes dos resultados de busca org√¢nicos
        if 'organic' in data:
            organic_results = data['organic'][:5]  # Primeiros 5 resultados
            sources = [r.get('link', '') for r in organic_results if r.get('link')]
            logger.info(f"üìã [SERPER] Encontrados {len(organic_results)} resultados org√¢nicos")
            
            # Usar LLM para extrair informa√ß√µes dos snippets
            if is_llm_available() and organic_results:
                snippets = [r.get('snippet', '') for r in organic_results if r.get('snippet')]
                snippets_text = '\n'.join(snippets[:3])  # Primeiros 3 snippets
                
                llm = get_llm(temperature=0.2)
                if llm:
                    prompt = f"""
                    Voc√™ √© um especialista em pesquisa de empresas para vendas B2B.
                    
                    Informa√ß√µes conhecidas do Lead:
                    - Nome: {lead_info.get('name', 'N/A')}
                    - Empresa: {company_name}
                    - Cargo: {lead_info.get('position', 'N/A')}
                    - Dom√≠nio: {domain}
                    
                    Snippets de resultados do Google sobre esta empresa:
                    {snippets_text}
                    
                    Com base nos snippets acima, extraia informa√ß√µes estruturadas sobre a empresa.
                    Retorne APENAS um JSON v√°lido com a seguinte estrutura:
                    {{
                        "phone": "telefone encontrado ou null",
                        "email": "email encontrado ou null",
                        "address": "endere√ßo completo ou null",
                        "city": "cidade ou null",
                        "state": "estado (sigla) ou null",
                        "zip_code": "CEP ou null",
                        "country": "pa√≠s ou 'Brasil'",
                        "industry": "setor/ind√∫stria da empresa",
                        "company_size": "tamanho estimado (ex: '50-200 funcion√°rios', 'Startup', 'Grande empresa')",
                        "context": "resumo completo da empresa incluindo: o que fazem, principais produtos/servi√ßos, tecnologias utilizadas, dores/pain points identificados, oportunidades de vendas, e qualquer informa√ß√£o relevante para prospec√ß√£o. Seja detalhado mas objetivo (m√≠nimo 200 palavras)."
                    }}
                    
                    IMPORTANTE:
                    - Se um campo n√£o for encontrado, retorne null
                    - Para telefone, use apenas n√∫meros e + se internacional
                    - Para CEP brasileiro, use formato 12345-678
                    - Para estado, use sigla (SP, RJ, MG, etc)
                    - O campo "context" √© cr√≠tico e deve ser um texto completo e detalhado
                    - Seja preciso e extraia apenas informa√ß√µes que podem ser inferidas dos snippets
                    """
                    
                    try:
                        llm_response = llm.invoke(prompt)
                        content = llm_response.content.strip()
                        
                        # Remover markdown code blocks se existirem
                        if '```json' in content:
                            content = content.split('```json')[1].split('```')[0]
                        elif '```' in content:
                            content = content.split('```')[1].split('```')[0]
                        
                        extracted = json.loads(content)
                        
                        # Mesclar dados extra√≠dos (priorizar dados do Knowledge Graph)
                        for key, value in extracted.items():
                            if value and (key not in enriched_data or not enriched_data.get(key)):
                                enriched_data[key] = value
                        
                        logger.info(f"‚úÖ [SERPER] LLM extraiu {len(extracted)} campos dos snippets")
                    except Exception as llm_error:
                        logger.warning(f"‚ö†Ô∏è [SERPER] Erro ao processar com LLM: {llm_error}")
        
        # Limpar valores None
        enriched_data = {k: v for k, v in enriched_data.items() if v}
        
        if enriched_data:
            logger.info(f"‚úÖ [SERPER] Encontradas informa√ß√µes: {list(enriched_data.keys())}")
            return {
                'success': True,
                'enriched_data': enriched_data,
                'method': 'serper',
                'sources': sources[:5]
            }
        else:
            return {'success': False, 'error': 'Nenhuma informa√ß√£o estruturada encontrada no Serper'}
            
    except Exception as e:
        logger.error(f"‚ùå [SERPER] Erro na fun√ß√£o enrich_via_serper_api: {e}")
        import traceback
        logger.error(f"‚ùå [SERPER] Traceback completo: {traceback.format_exc()}")
        return {'success': False, 'error': f'Erro na API Serper: {str(e)}'}


async def enrich_via_rapidapi_linkedin(linkedin_url: str, lead_info: Dict[str, Any]) -> Dict[str, Any]:
    """Enriquece lead usando RapidAPI para extrair dados do LinkedIn"""
    logger.info(f"üîç [RAPIDAPI LINKEDIN] Fun√ß√£o chamada. URL: {linkedin_url}")
    logger.info(f"üîç [RAPIDAPI LINKEDIN] RapidAPI key presente: {settings.rapidapi_key is not None}")
    logger.info(f"üîç [RAPIDAPI LINKEDIN] Host configurado: {settings.rapidapi_linkedin_host}")
    
    if not settings.rapidapi_key:
        logger.warning("‚ö†Ô∏è [RAPIDAPI LINKEDIN] API key n√£o configurada")
        return {'success': False, 'error': 'RapidAPI key n√£o configurada'}
    
    if not linkedin_url:
        logger.warning("‚ö†Ô∏è [RAPIDAPI LINKEDIN] URL do LinkedIn n√£o fornecida")
        return {'success': False, 'error': 'URL do LinkedIn n√£o fornecida'}
    
    logger.info(f"üîç [RAPIDAPI LINKEDIN] Buscando informa√ß√µes do LinkedIn: {linkedin_url}")
    
    try:
        enriched_data = {}
        sources = []
        
        # Extrair username ou ID do LinkedIn da URL
        # Exemplos: linkedin.com/in/username, linkedin.com/company/company-name
        linkedin_username = None
        if '/in/' in linkedin_url:
            linkedin_username = linkedin_url.split('/in/')[-1].split('/')[0].split('?')[0]
        elif '/company/' in linkedin_url:
            linkedin_username = linkedin_url.split('/company/')[-1].split('/')[0].split('?')[0]
        
        if not linkedin_username:
            logger.warning(f"‚ö†Ô∏è [RAPIDAPI LINKEDIN] N√£o foi poss√≠vel extrair username da URL: {linkedin_url}")
            return {'success': False, 'error': 'URL do LinkedIn inv√°lida. Use formato: https://www.linkedin.com/in/username'}
        
        logger.info(f"üìã [RAPIDAPI LINKEDIN] Username extra√≠do: {linkedin_username}")
        
        # Usar a API do RapidAPI para LinkedIn
        # Nota: O endpoint pode variar dependendo da API escolhida no RapidAPI
        # linkedin-api8 geralmente usa POST com a URL completa no body
        headers = {
            'X-RapidAPI-Key': settings.rapidapi_key,
            'X-RapidAPI-Host': settings.rapidapi_linkedin_host,
            'Content-Type': 'application/json'
        }
        
        logger.info(f"üìã [RAPIDAPI LINKEDIN] Headers: X-RapidAPI-Key={'*' * 10}... (oculto por seguran√ßa)")
        logger.info(f"üìã [RAPIDAPI LINKEDIN] Host: {settings.rapidapi_linkedin_host}")
        
        # Formato correto baseado na documenta√ß√£o: GET com username como query parameter
        # curl --request GET --url 'https://linkedin-data-api.p.rapidapi.com/?username=adamselipsky'
        endpoint = settings.rapidapi_linkedin_endpoint if settings.rapidapi_linkedin_endpoint else "/"
        
        # Se endpoint customizado configurado, usar apenas ele
        if settings.rapidapi_linkedin_endpoint:
            logger.info(f"üìã [RAPIDAPI LINKEDIN] Usando endpoint customizado: {settings.rapidapi_linkedin_endpoint}")
            endpoints_to_try = [
                {
                    'method': 'GET',
                    'url': f"https://{settings.rapidapi_linkedin_host}{settings.rapidapi_linkedin_endpoint}",
                    'payload': None,
                    'params': {'username': linkedin_username},
                    'description': f'GET {settings.rapidapi_linkedin_endpoint}?username=...'
                },
            ]
        else:
            # Formato padr√£o baseado na documenta√ß√£o oficial
            endpoints_to_try = [
                # Formato oficial: GET com username como query parameter
                {
                    'method': 'GET',
                    'url': f"https://{settings.rapidapi_linkedin_host}/",
                    'payload': None,
                    'params': {'username': linkedin_username},
                    'description': 'GET /?username=... (formato oficial)'
                },
                # Fallback: tentar outros formatos comuns
                {
                    'method': 'GET',
                    'url': f"https://{settings.rapidapi_linkedin_host}/profile",
                    'payload': None,
                    'params': {'username': linkedin_username},
                    'description': 'GET /profile?username=...'
                },
                {
                    'method': 'POST',
                    'url': f"https://{settings.rapidapi_linkedin_host}/",
                    'payload': {'username': linkedin_username},
                    'description': 'POST / com username no body'
                },
            ]
        
        last_error = None
        for endpoint_config in endpoints_to_try:
            try:
                method = endpoint_config['method']
                api_url = endpoint_config['url']
                payload = endpoint_config.get('payload')
                params = endpoint_config.get('params')
                description = endpoint_config['description']
                
                logger.info(f"üìã [RAPIDAPI LINKEDIN] Tentando {description}: {method} {api_url}")
                
                if method == 'POST':
                    if payload:
                        logger.info(f"üìã [RAPIDAPI LINKEDIN] Payload: {payload}")
                        response = requests.post(api_url, json=payload, headers=headers, timeout=15)
                    else:
                        response = requests.post(api_url, headers=headers, timeout=15)
                else:  # GET
                    if params:
                        response = requests.get(api_url, params=params, headers=headers, timeout=15)
                    else:
                        response = requests.get(api_url, headers=headers, timeout=15)
                
                logger.info(f"üì° [RAPIDAPI LINKEDIN] Resposta recebida ({description}). Status: {response.status_code}")
                
                # Log do conte√∫do da resposta (primeiros 500 caracteres)
                response_text = response.text[:500]
                logger.info(f"üì° [RAPIDAPI LINKEDIN] Conte√∫do da resposta (primeiros 500 chars): {response_text}")
                
                # Se sucesso HTTP (2xx), verificar o conte√∫do da resposta
                if 200 <= response.status_code < 300:
                    data = response.json()
                    logger.info(f"‚úÖ [RAPIDAPI LINKEDIN] {description} retornou HTTP 200. Keys: {list(data.keys())}")
                    logger.info(f"‚úÖ [RAPIDAPI LINKEDIN] Dados recebidos (primeiros 1000 chars): {json.dumps(data, indent=2, ensure_ascii=False)[:1000]}")
                    
                    # Verificar se a resposta indica sucesso ou falha
                    if data.get('success') is False:
                        error_message = data.get('message', 'Servi√ßo n√£o dispon√≠vel')
                        logger.warning(f"‚ö†Ô∏è [RAPIDAPI LINKEDIN] API retornou success=false: {error_message}")
                        
                        # Se a mensagem indica que o servi√ßo n√£o est√° mais dispon√≠vel
                        if 'no longer providing' in error_message.lower() or 'not available' in error_message.lower():
                            error_msg = f'A API do LinkedIn n√£o est√° mais dispon√≠vel: {error_message}. Tente usar outra API do RapidAPI ou configure uma alternativa.'
                            logger.error(f"‚ùå [RAPIDAPI LINKEDIN] {error_msg}")
                            return {'success': False, 'error': error_msg, 'api_unavailable': True}
                        
                        # Outro tipo de erro na resposta
                        last_error = f'API retornou success=false: {error_message}'
                        logger.warning(f"‚ö†Ô∏è [RAPIDAPI LINKEDIN] {last_error}. Tentando pr√≥ximo formato...")
                        continue
                    
                    # Se chegou aqui, a resposta indica sucesso
                    # Sair do loop - encontramos o formato correto e os dados
                    break
                elif response.status_code == 404:
                    # 404 pode significar endpoint incorreto, tentar pr√≥ximo
                    error_detail = response.text[:500] if response.text else "Sem detalhes"
                    logger.warning(f"‚ö†Ô∏è [RAPIDAPI LINKEDIN] {description} retornou 404: {error_detail}. Tentando pr√≥ximo formato...")
                    last_error = f'Endpoint n√£o encontrado: {error_detail}'
                    continue
                else:
                    # Outro erro HTTP, tentar pr√≥ximo formato
                    error_detail = response.text[:500] if response.text else "Sem detalhes"
                    logger.warning(f"‚ö†Ô∏è [RAPIDAPI LINKEDIN] {description} retornou {response.status_code}: {error_detail}. Tentando pr√≥ximo formato...")
                    last_error = f'Erro HTTP {response.status_code}: {error_detail}'
                    continue
                    
            except requests.exceptions.RequestException as e:
                logger.warning(f"‚ö†Ô∏è [RAPIDAPI LINKEDIN] {description} falhou com exce√ß√£o: {str(e)}. Tentando pr√≥ximo formato...")
                last_error = f'Erro na requisi√ß√£o: {str(e)}'
                continue
        
        # Se chegou aqui, nenhum formato funcionou
        if 'data' not in locals():
            error_msg = f'''Todos os formatos de endpoint falharam. O endpoint '/profile' n√£o existe nesta API.

Para resolver:
1. Acesse a p√°gina da API no RapidAPI (https://rapidapi.com)
2. Encontre a se√ß√£o "Endpoints" ou "Documentation"
3. Identifique o endpoint correto (ex: /v1/profile, /api/profile, etc.)
4. Configure no .env: RAPIDAPI_LINKEDIN_ENDPOINT=/endpoint-correto

√öltimo erro: {last_error if last_error else "desconhecido"}

Veja o arquivo ENCONTRAR_ENDPOINT_RAPIDAPI.md para instru√ß√µes detalhadas.'''
            logger.error(f"‚ùå [RAPIDAPI LINKEDIN] {error_msg}")
            return {'success': False, 'error': error_msg}
        
        # Extrair informa√ß√µes do perfil do LinkedIn
        # A estrutura pode variar dependendo da API do RapidAPI usada
        # Vou criar uma estrutura gen√©rica que funciona com diferentes formatos
        
        # Nome completo
        if 'fullName' in data or 'name' in data:
            # J√° temos o nome do lead, mas podemos validar
            pass
        
        # Headline/T√≠tulo Profissional
        if 'headline' in data:
            enriched_data['linkedin_headline'] = data.get('headline', '')
            # Tamb√©m usar para position se n√£o tiver
            if not lead_info.get('position') or lead_info.get('position') == '':
                enriched_data['position'] = data.get('headline', '')
        
        # Sobre/About
        if 'summary' in data or 'about' in data:
            summary = data.get('summary') or data.get('about', '')
            if summary:
                enriched_data['linkedin_about'] = summary
                # Adicionar ao contexto se j√° existir, ou criar novo
                current_context = lead_info.get('context', '')
                if current_context:
                    enriched_data['context'] = f"{current_context}\n\nInforma√ß√µes do LinkedIn:\n{summary}"
                else:
                    enriched_data['context'] = f"Informa√ß√µes do LinkedIn:\n{summary}"
        
        # Localiza√ß√£o
        if 'location' in data:
            location = data.get('location', '')
            if location:
                # Tentar extrair cidade e estado
                location_parts = location.split(',')
                if len(location_parts) >= 2:
                    enriched_data['city'] = location_parts[0].strip()
                    enriched_data['state'] = location_parts[1].strip()
                else:
                    enriched_data['city'] = location
        
        # Empresa atual
        if 'currentCompany' in data:
            company = data.get('currentCompany', {})
            if isinstance(company, dict):
                if 'name' in company and not lead_info.get('company'):
                    enriched_data['company'] = company.get('name', '')
                if 'industry' in company:
                    enriched_data['industry'] = company.get('industry', '')
                if 'size' in company:
                    enriched_data['company_size'] = company.get('size', '')
        elif 'company' in data:
            if not lead_info.get('company'):
                enriched_data['company'] = data.get('company', '')
        
        # Experi√™ncias profissionais - salvar em JSON
        experiences = []
        if 'experiences' in data:
            experiences = data.get('experiences', [])
        elif 'experience' in data:
            experiences = data.get('experience', [])
        
        if experiences:
            # Normalizar formato das experi√™ncias
            normalized_experiences = []
            exp_list = experiences if isinstance(experiences, list) else [experiences]
            for exp in exp_list:
                if isinstance(exp, dict):
                    normalized_exp = {
                        'position': exp.get('title') or exp.get('position') or exp.get('jobTitle', ''),
                        'company': exp.get('company') or exp.get('companyName') or exp.get('company_name', ''),
                        'start_date': exp.get('startDate') or exp.get('start_date') or exp.get('start', ''),
                        'end_date': exp.get('endDate') or exp.get('end_date') or exp.get('end') or None,
                        'description': exp.get('description') or exp.get('summary', '')
                    }
                    normalized_experiences.append(normalized_exp)
            
            if normalized_experiences:
                enriched_data['linkedin_experience_json'] = json.dumps(normalized_experiences, ensure_ascii=False)
            
            # Usar a experi√™ncia mais recente para enriquecer campos b√°sicos
            latest_exp = exp_list[0] if exp_list else None
            if isinstance(latest_exp, dict):
                if 'company' in latest_exp and not lead_info.get('company'):
                    enriched_data['company'] = latest_exp.get('company', '') or latest_exp.get('companyName', '')
                if 'title' in latest_exp and not lead_info.get('position'):
                    enriched_data['position'] = latest_exp.get('title', '') or latest_exp.get('position', '')
                if 'industry' in latest_exp:
                    enriched_data['industry'] = latest_exp.get('industry', '')
        
        # Educa√ß√£o - salvar em JSON
        if 'education' in data:
            education = data.get('education', [])
            if education:
                normalized_education = []
                edu_list = education if isinstance(education, list) else [education]
                for edu in edu_list:
                    if isinstance(edu, dict):
                        normalized_edu = {
                            'institution': edu.get('school') or edu.get('institution') or edu.get('schoolName', ''),
                            'degree': edu.get('degree') or edu.get('fieldOfStudy') or edu.get('field_of_study', ''),
                            'field': edu.get('fieldOfStudy') or edu.get('field_of_study') or edu.get('major', ''),
                            'start_date': edu.get('startDate') or edu.get('start_date') or edu.get('start', ''),
                            'end_date': edu.get('endDate') or edu.get('end_date') or edu.get('end') or None
                        }
                        normalized_education.append(normalized_edu)
                
                if normalized_education:
                    enriched_data['linkedin_education_json'] = json.dumps(normalized_education, ensure_ascii=False)
        
        # Certifica√ß√µes - salvar em JSON
        if 'certifications' in data:
            certifications = data.get('certifications', [])
            if certifications:
                normalized_certs = []
                cert_list = certifications if isinstance(certifications, list) else [certifications]
                for cert in cert_list:
                    if isinstance(cert, dict):
                        normalized_cert = {
                            'name': cert.get('name') or cert.get('title') or cert.get('certificationName', ''),
                            'issuer': cert.get('issuer') or cert.get('issuingOrganization') or cert.get('issuing_organization', ''),
                            'issue_date': cert.get('issueDate') or cert.get('issue_date') or cert.get('issued', ''),
                            'expiration_date': cert.get('expirationDate') or cert.get('expiration_date') or None,
                            'credential_id': cert.get('credentialId') or cert.get('credential_id', '')
                        }
                        normalized_certs.append(normalized_cert)
                
                if normalized_certs:
                    enriched_data['linkedin_certifications_json'] = json.dumps(normalized_certs, ensure_ascii=False)
        
        # Habilidades/Skills
        if 'skills' in data:
            skills = data.get('skills', [])
            if skills:
                if isinstance(skills, list):
                    skills_text = ', '.join(skills)
                else:
                    skills_text = str(skills)
                enriched_data['linkedin_skills'] = skills_text
                # Adicionar ao contexto tamb√©m
                current_context = enriched_data.get('context', lead_info.get('context', ''))
                if current_context:
                    enriched_data['context'] = f"{current_context}\n\nHabilidades: {skills_text}"
                else:
                    enriched_data['context'] = f"Habilidades: {skills_text}"
        
        # Artigos/Publica√ß√µes - salvar em JSON
        if 'articles' in data or 'publications' in data:
            articles = data.get('articles') or data.get('publications', [])
            if articles:
                normalized_articles = []
                art_list = articles if isinstance(articles, list) else [articles]
                for article in art_list:
                    if isinstance(article, dict):
                        normalized_article = {
                            'title': article.get('title') or article.get('name', ''),
                            'url': article.get('url') or article.get('link', ''),
                            'published_date': article.get('publishedDate') or article.get('published_date') or article.get('date', ''),
                            'description': article.get('description') or article.get('summary', '')
                        }
                        normalized_articles.append(normalized_article)
                
                if normalized_articles:
                    enriched_data['linkedin_articles_json'] = json.dumps(normalized_articles, ensure_ascii=False)
        
        # Conex√µes e Seguidores
        if 'connections' in data:
            connections = data.get('connections')
            if isinstance(connections, (int, str)):
                try:
                    enriched_data['linkedin_connections_count'] = int(connections)
                except (ValueError, TypeError):
                    pass
        
        if 'followers' in data or 'followersCount' in data:
            followers = data.get('followers') or data.get('followersCount')
            if isinstance(followers, (int, str)):
                try:
                    enriched_data['linkedin_followers_count'] = int(followers)
                except (ValueError, TypeError):
                    pass
        
        # Atividades recentes (pode ser um resumo das √∫ltimas atividades)
        if 'recentActivity' in data or 'activities' in data:
            activity = data.get('recentActivity') or data.get('activities', '')
            if activity:
                if isinstance(activity, list):
                    activity_text = '\n'.join([str(a) for a in activity[:5]])  # √öltimas 5 atividades
                else:
                    activity_text = str(activity)
                enriched_data['linkedin_recent_activity'] = activity_text
        
        # Telefone (raramente dispon√≠vel no LinkedIn p√∫blico, mas verificar)
        if 'phone' in data:
            enriched_data['phone'] = data.get('phone', '')
        
        # Email (raramente dispon√≠vel no LinkedIn p√∫blico, mas verificar)
        if 'email' in data and not lead_info.get('email'):
            enriched_data['email'] = data.get('email', '')
        
        # Limpar valores None ou vazios
        enriched_data = {k: v for k, v in enriched_data.items() if v and v != ''}
        
        # Adicionar fonte
        sources.append(linkedin_url)
        
        if enriched_data:
            logger.info(f"‚úÖ [RAPIDAPI LINKEDIN] Encontradas informa√ß√µes: {list(enriched_data.keys())}")
            return {
                'success': True,
                'enriched_data': enriched_data,
                'method': 'rapidapi_linkedin',
                'sources': sources
            }
        else:
            logger.warning("‚ö†Ô∏è [RAPIDAPI LINKEDIN] Nenhuma informa√ß√£o estruturada encontrada")
            return {'success': False, 'error': 'Nenhuma informa√ß√£o estruturada encontrada no LinkedIn'}
            
    except Exception as e:
        logger.error(f"‚ùå [RAPIDAPI LINKEDIN] Erro na fun√ß√£o enrich_via_rapidapi_linkedin: {e}")
        import traceback
        logger.error(f"‚ùå [RAPIDAPI LINKEDIN] Traceback completo: {traceback.format_exc()}")
        return {'success': False, 'error': f'Erro ao processar RapidAPI LinkedIn: {str(e)}'}


async def research_lead_website_with_fallback(lead_website: str, lead_info: Dict[str, Any]) -> Dict[str, Any]:
    """Pesquisa com m√∫ltiplas estrat√©gias de fallback em cascata"""
    logger.info(f"üîç [RESEARCHER] Iniciando pesquisa com fallback para: {lead_website}")
    
    company_name = lead_info.get('company', '')
    domain = lead_website.replace('https://', '').replace('http://', '').split('/')[0]
    
    # ESTRAT√âGIA 1: Scraping direto do website
    logger.info("üìã [ESTRAT√âGIA 1] Tentando scraping direto...")
    website_data = await scrape_website(lead_website)
    
    # Verificar se scraping foi bem-sucedido
    # Sucesso = n√£o tem 'success': False E n√£o tem 'error' E tem 'soup' ou 'main_texts'
    has_error = website_data.get('success') is False or 'error' in website_data
    has_content = website_data.get('soup') is not None or len(website_data.get('main_texts', [])) > 0
    
    logger.info(f"üîç [RESEARCHER] Verifica√ß√£o de sucesso: has_error={has_error}, has_content={has_content}")
    logger.debug(f"üîç [RESEARCHER] website_data keys: {list(website_data.keys())}")
    
    if not has_error and has_content:
        logger.info("‚úÖ [ESTRAT√âGIA 1] Scraping direto bem-sucedido!")
        soup = website_data.get('soup')
        contact_info = extract_contact_info(soup) if soup else {}
        logger.info(f"üìû [RESEARCHER] Informa√ß√µes de contato extra√≠das: {list(contact_info.keys())}")
        
        enriched_data = await enrich_lead_data_with_llm(website_data, contact_info, lead_info)
        logger.info(f"‚ú® [RESEARCHER] Dados enriquecidos: {list(enriched_data.keys())}")
        
        analysis = await analyze_website_content(website_data, lead_info)
        
        return {
            'success': True,
            'url': lead_website,
            'enriched_data': enriched_data,
            'analysis': analysis,
            'method': 'direct_scraping',
            'timestamp': datetime.utcnow().isoformat()
        }
    
    # Se bloqueado (403) ou outro erro, tentar estrat√©gias alternativas
    status_code = website_data.get('status_code')
    error_msg = website_data.get('error', 'Erro desconhecido')
    logger.warning(f"‚ö†Ô∏è [ESTRAT√âGIA 1] Falhou. Status: {status_code}, Erro: {error_msg}. Tentando estrat√©gias alternativas...")
    
    # ESTRAT√âGIA 2: Serper.dev API (mais confi√°vel que scraping do Google)
    logger.info(f"üîç [DEBUG] Serper.dev API key configurada: {settings.serper_api_key is not None}")
    if settings.serper_api_key:
        logger.info("üìã [ESTRAT√âGIA 2] Tentando Serper.dev API...")
        try:
            serper_result = await enrich_via_serper_api(company_name, domain, lead_info)
            logger.info(f"üìä [ESTRAT√âGIA 2] Resultado do Serper.dev: success={serper_result.get('success')}, error={serper_result.get('error', 'N/A')}")
            if serper_result.get('success'):
                logger.info("‚úÖ [ESTRAT√âGIA 2] Serper.dev bem-sucedido!")
                return {
                    'success': True,
                    'url': lead_website,
                    'enriched_data': serper_result.get('enriched_data', {}),
                    'analysis': {'method': 'serper', 'sources': serper_result.get('sources', [])},
                    'method': 'serper',
                    'timestamp': datetime.utcnow().isoformat()
                }
            error_detail = serper_result.get('error', 'Erro desconhecido')
            logger.warning(f"‚ö†Ô∏è [ESTRAT√âGIA 2] Falhou: {error_detail}")
        except Exception as e:
            logger.error(f"‚ùå [ESTRAT√âGIA 2] Erro ao executar Serper.dev: {e}")
            import traceback
            traceback.print_exc()
    else:
        logger.info("‚ö†Ô∏è [ESTRAT√âGIA 2] Serper.dev API key n√£o configurada. Pulando...")
    
    # ESTRAT√âGIA 2.5: RapidAPI LinkedIn (enriquecimento com dados profissionais)
    linkedin_url = lead_info.get('linkedin_url', '')
    logger.info(f"üîç [DEBUG] RapidAPI key configurada: {settings.rapidapi_key is not None}")
    logger.info(f"üîç [DEBUG] LinkedIn URL dispon√≠vel: {linkedin_url is not None and linkedin_url != ''}")
    
    if settings.rapidapi_key and linkedin_url:
        logger.info("üìã [ESTRAT√âGIA 2.5] Tentando RapidAPI LinkedIn...")
        try:
            linkedin_result = await enrich_via_rapidapi_linkedin(linkedin_url, lead_info)
            logger.info(f"üìä [ESTRAT√âGIA 2.5] Resultado do RapidAPI LinkedIn: success={linkedin_result.get('success')}, error={linkedin_result.get('error', 'N/A')}")
            if linkedin_result.get('success'):
                logger.info("‚úÖ [ESTRAT√âGIA 2.5] RapidAPI LinkedIn bem-sucedido!")
                return {
                    'success': True,
                    'url': lead_website,
                    'enriched_data': linkedin_result.get('enriched_data', {}),
                    'analysis': {'method': 'rapidapi_linkedin', 'sources': linkedin_result.get('sources', [])},
                    'method': 'rapidapi_linkedin',
                    'timestamp': datetime.utcnow().isoformat()
                }
            error_detail = linkedin_result.get('error', 'Erro desconhecido')
            logger.warning(f"‚ö†Ô∏è [ESTRAT√âGIA 2.5] Falhou: {error_detail}")
        except Exception as e:
            logger.error(f"‚ùå [ESTRAT√âGIA 2.5] Erro ao executar RapidAPI LinkedIn: {e}")
            import traceback
            traceback.print_exc()
    else:
        if not settings.rapidapi_key:
            logger.info("‚ö†Ô∏è [ESTRAT√âGIA 2.5] RapidAPI key n√£o configurada. Pulando...")
        if not linkedin_url:
            logger.info("‚ö†Ô∏è [ESTRAT√âGIA 2.5] LinkedIn URL n√£o dispon√≠vel. Pulando...")
    
    # ESTRAT√âGIA 3: Google Search + LLM (fallback gratuito)
    logger.info(f"üîç [DEBUG] Google Search dispon√≠vel: {GOOGLE_SEARCH_AVAILABLE}")
    logger.info(f"üîç [DEBUG] LLM dispon√≠vel: {is_llm_available()}")
    logger.info(f"üîç [DEBUG] Provedor LLM: {settings.llm_provider}")
    
    if GOOGLE_SEARCH_AVAILABLE:
        if is_llm_available():
            logger.info("üìã [ESTRAT√âGIA 3] Tentando Google Search + LLM...")
            try:
                google_result = await enrich_via_google_search(company_name, domain, lead_info)
                if google_result.get('success'):
                    logger.info("‚úÖ [ESTRAT√âGIA 3] Google Search bem-sucedido!")
                    return {
                        'success': True,
                        'url': lead_website,
                        'enriched_data': google_result.get('enriched_data', {}),
                        'analysis': {'method': 'google_search', 'sources': google_result.get('sources', [])},
                        'method': 'google_search',
                        'timestamp': datetime.utcnow().isoformat()
                    }
                error_detail = google_result.get('error', 'Erro desconhecido')
                logger.warning(f"‚ö†Ô∏è [ESTRAT√âGIA 3] Falhou: {error_detail}")
                # Adicionar mais contexto sobre o erro
                if 'LLM n√£o' in error_detail:
                    logger.info(f"üí° [ESTRAT√âGIA 3] Configure LLM: LLM_PROVIDER=ollama ou LLM_PROVIDER=openai")
                elif 'Nenhum resultado' in error_detail:
                    logger.info(f"üí° [ESTRAT√âGIA 3] Nenhum resultado encontrado no Google para: {company_name}")
            except Exception as e:
                logger.error(f"‚ùå [ESTRAT√âGIA 3] Erro ao executar Google Search: {e}")
                import traceback
                traceback.print_exc()
        else:
            logger.warning(f"‚ö†Ô∏è [ESTRAT√âGIA 3] Google Search dispon√≠vel mas LLM n√£o configurado (provedor: {settings.llm_provider}). Pulando...")
            logger.info(f"üí° [ESTRAT√âGIA 3] Configure LLM no .env: LLM_PROVIDER=ollama ou LLM_PROVIDER=openai com OPENAI_API_KEY")
    else:
        logger.warning("‚ö†Ô∏è [ESTRAT√âGIA 3] Google Search n√£o dispon√≠vel (biblioteca n√£o instalada). Pulando...")
    
    # ESTRAT√âGIA 4: Hunter.io API
    if settings.hunter_api_key:
        logger.info("üìã [ESTRAT√âGIA 4] Tentando Hunter.io API...")
        hunter_result = await enrich_via_hunter_api(domain, company_name)
        if hunter_result.get('success'):
            logger.info("‚úÖ [ESTRAT√âGIA 4] Hunter.io bem-sucedido!")
            return {
                'success': True,
                'url': lead_website,
                'enriched_data': hunter_result.get('enriched_data', {}),
                'analysis': {'method': 'hunter_io', 'emails_found': hunter_result.get('emails_found', [])},
                'method': 'hunter_io',
                'timestamp': datetime.utcnow().isoformat()
            }
        logger.warning(f"‚ö†Ô∏è [ESTRAT√âGIA 4] Falhou: {hunter_result.get('error')}")
    
    # ESTRAT√âGIA 6: Clearbit API
    if settings.clearbit_api_key:
        logger.info("üìã [ESTRAT√âGIA 6] Tentando Clearbit API...")
        clearbit_result = await enrich_via_clearbit_api(domain)
        if clearbit_result.get('success'):
            logger.info("‚úÖ [ESTRAT√âGIA 5] Clearbit bem-sucedido!")
            return {
                'success': True,
                'url': lead_website,
                'enriched_data': clearbit_result.get('enriched_data', {}),
                'analysis': {'method': 'clearbit'},
                'method': 'clearbit',
                'timestamp': datetime.utcnow().isoformat()
            }
        logger.warning(f"‚ö†Ô∏è [ESTRAT√âGIA 5] Falhou: {clearbit_result.get('error')}")
    
    # Se todas as estrat√©gias falharam
    logger.error("‚ùå [RESEARCHER] Todas as estrat√©gias falharam")
    
    # Construir mensagem detalhada sobre o que foi tentado
    attempted_strategies = ["Scraping Direto"]
    
    if settings.serper_api_key:
        attempted_strategies.append("Serper.dev (tentado)")
    else:
        attempted_strategies.append("Serper.dev (n√£o configurado)")
    
    linkedin_url = lead_info.get('linkedin_url', '')
    if settings.rapidapi_key and linkedin_url:
        attempted_strategies.append("RapidAPI LinkedIn (tentado)")
    elif settings.rapidapi_key:
        attempted_strategies.append("RapidAPI LinkedIn (URL n√£o dispon√≠vel)")
    else:
        attempted_strategies.append("RapidAPI LinkedIn (n√£o configurado)")
    
    google_status = ""
    
    if GOOGLE_SEARCH_AVAILABLE:
        if is_llm_available():
            attempted_strategies.append("Google Search + LLM (tentado)")
            google_status = "tentado mas falhou"
        else:
            attempted_strategies.append("Google Search (LLM n√£o configurado)")
            google_status = "n√£o executado - LLM n√£o configurado"
    else:
        attempted_strategies.append("Google Search (biblioteca n√£o instalada)")
        google_status = "n√£o dispon√≠vel"
    
    if settings.hunter_api_key:
        attempted_strategies.append("Hunter.io")
    else:
        attempted_strategies.append("Hunter.io (n√£o configurado)")
    
    if settings.clearbit_api_key:
        attempted_strategies.append("Clearbit")
    else:
        attempted_strategies.append("Clearbit (n√£o configurado)")
    
    error_message = f'Scraping direto bloqueado (status {status_code}). Estrat√©gias tentadas: {", ".join(attempted_strategies)}.'
    
    suggestions = []
    
    # Sugest√µes espec√≠ficas baseadas no que est√° faltando
    if not settings.serper_api_key:
        suggestions.append('Configure Serper.dev API key no arquivo .env (SERPER_API_KEY=sua-chave) - Mais confi√°vel e r√°pido que scraping')
    
    if not GOOGLE_SEARCH_AVAILABLE:
        suggestions.append('Instale googlesearch-python: pip install googlesearch-python')
    
    if not is_llm_available():
        suggestions.append(f'Configure LLM no arquivo .env:')
        suggestions.append('  - Para Ollama (gratuito/local): LLM_PROVIDER=ollama OLLAMA_MODEL=llama3')
        suggestions.append('  - Para OpenAI: LLM_PROVIDER=openai OPENAI_API_KEY=sua-chave')
        suggestions.append('  - Instale Ollama: brew install ollama && ollama pull llama3')
    else:
        suggestions.append('Google Search + LLM est√° configurado mas falhou. Verifique os logs para mais detalhes.')
    
    if not settings.hunter_api_key and not settings.clearbit_api_key:
        suggestions.append('Configure Hunter.io ou Clearbit API keys no arquivo .env para fallback autom√°tico')
    
    suggestions.append('Acesse o website manualmente para coletar informa√ß√µes')
    
    return {
        'success': False,
        'error': error_message,
        'url': lead_website,
        'status_code': status_code,
        'suggestions': suggestions,
        'attempted_strategies': attempted_strategies
    }


async def research_lead_website(lead_website: str, lead_info: Dict[str, Any]) -> Dict[str, Any]:
    """Pesquisa completa do website do lead e enriquecimento de dados (usa fallback autom√°tico)"""
    return await research_lead_website_with_fallback(lead_website, lead_info)