import logging
from typing import Optional
from langchain_openai import ChatOpenAI
from app.config import settings

# Configurar logger
logger = logging.getLogger(__name__)

# Importar Ollama apenas se dispon√≠vel
try:
    from langchain_ollama import ChatOllama
    OLLAMA_AVAILABLE = True
except ImportError:
    OLLAMA_AVAILABLE = False
    logger.warning("‚ö†Ô∏è langchain-ollama n√£o instalado. Ollama n√£o dispon√≠vel.")


def get_llm(temperature: float = 0.2, model_override: Optional[str] = None):
    """
    Retorna uma inst√¢ncia do LLM configurado (OpenAI ou Ollama)
    
    Args:
        temperature: Temperatura para o modelo (padr√£o: 0.2)
        model_override: Sobrescrever o modelo padr√£o (opcional)
    
    Returns:
        Inst√¢ncia do LLM ou None se n√£o configurado
    """
    provider = settings.llm_provider.lower()
    
    if provider == "ollama":
        if not OLLAMA_AVAILABLE:
            logger.error("‚ùå [LLM] Ollama selecionado mas langchain-ollama n√£o est√° instalado")
            logger.info("üí° [LLM] Instale com: pip install langchain-ollama")
            return None
        
        model = model_override or settings.ollama_model
        logger.info(f"ü§ñ [LLM] Usando Ollama - Modelo: {model}, URL: {settings.ollama_base_url}")
        try:
            return ChatOllama(
                model=model,
                base_url=settings.ollama_base_url,
                temperature=temperature,
                num_ctx=4096  # Contexto maior para melhor an√°lise
            )
        except Exception as e:
            logger.error(f"‚ùå [LLM] Erro ao inicializar Ollama: {e}")
            logger.info(f"üí° [LLM] Certifique-se de que o Ollama est√° rodando em {settings.ollama_base_url}")
            return None
    
    elif provider == "openai":
        if not settings.openai_api_key:
            logger.warning("‚ö†Ô∏è [LLM] OpenAI selecionado mas API key n√£o configurada")
            return None
        
        model = model_override or settings.openai_model
        logger.info(f"ü§ñ [LLM] Usando OpenAI - Modelo: {model}")
        return ChatOpenAI(
            model=model,
            temperature=temperature,
            openai_api_key=settings.openai_api_key
        )
    
    else:
        logger.error(f"‚ùå [LLM] Provedor desconhecido: {provider}. Use 'openai' ou 'ollama'")
        return None


def is_llm_available() -> bool:
    """Verifica se algum LLM est√° dispon√≠vel"""
    llm = get_llm()
    return llm is not None

